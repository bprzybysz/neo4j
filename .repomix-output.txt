This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-08T19:21:00.062Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
movie_graph/
  db/
    connection.py
  etl/
    process.py
    README.md
  utils/
    helpers.py
  __init__.py
notebooks/
  cypher/
    analysis_queries.cypher
    import_data.cypher
  movie_analysis_updated.ipynb
  movie_analysis.ipynb
  neo4j_template.ipynb
scripts/
  convert_notebook.py
  create_notebook_template.py
  debug_extract.py
  download_data.py
  etl_process.py
  fix_notebooks.py
  requirements.txt
  run_with_checks.py
  test_etl_process.py
  validate_notebooks.py
tests/
  test_db.py
  test_etl.py
workflow/
  codemap.md
  plan.md
  progression.md
  summary.md
.cursorignore
.cursorrules
.gitignore
check_environment.sh
data_model.md
README.md
requirements.txt
run_tests.sh
setup_venv.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="movie_graph/db/connection.py">
"""
Neo4j database connection and query utilities.
"""
from typing import Dict, List, Optional, Tuple, Any, Union
import pandas as pd
from neo4j import GraphDatabase
class Neo4jConnection:
    """
    Handles Neo4j database connections and queries.
    This class provides a simple interface for connecting to Neo4j,
    executing Cypher queries, and returning results as DataFrames.
    """
    def __init__(self, uri: str, auth: Tuple[str, str], database: Optional[str] = None):
        """
        Initialize a new Neo4j connection.
        Args:
            uri: The Neo4j connection URI (e.g., "bolt://localhost:7687")
            auth: A tuple containing (username, password)
            database: Optional database name to connect to
        """
        self.driver = GraphDatabase.driver(uri, auth=auth)
        self.database = database
    def close(self) -> None:
        """
        Close the Neo4j connection.
        """
        self.driver.close()
    def query(self, query: str, parameters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """
        Execute a Cypher query and return results as a pandas DataFrame.
        Args:
            query: The Cypher query string
            parameters: Optional parameters to pass to the query
        Returns:
            A pandas DataFrame containing the query results
        """
        if parameters is None:
            parameters = {}
        with self.driver.session(database=self.database) as session:
            result = session.run(query, parameters)
            return pd.DataFrame([r.values() for r in result], columns=result.keys())
    def __enter__(self) -> 'Neo4jConnection':
        """
        Support for using this class as a context manager.
        """
        return self
    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        """
        Close connection when exiting context.
        """
        self.close()
def get_connection(uri: str, auth: Tuple[str, str], database: Optional[str] = None) -> Neo4jConnection:
    """
    Create and return a Neo4j connection.
    A utility function to simplify creating a connection.
    Args:
        uri: The Neo4j connection URI
        auth: A tuple containing (username, password)
        database: Optional database name to connect to
    Returns:
        A Neo4jConnection instance
    """
    return Neo4jConnection(uri, auth, database)
</file>

<file path="movie_graph/etl/process.py">
"""
ETL process for transforming TMDB movie dataset into Neo4j importable format.
"""
import os
import json
from typing import Dict, List, Any, Union, Optional
import pandas as pd
def ensure_dir(directory: str) -> None:
    """
    Create directory if it doesn't exist.
    Args:
        directory: Path to directory to create
    """
    if not os.path.exists(directory):
        os.makedirs(directory)
def parse_json_fields(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """
    Parse JSON strings in DataFrame columns.
    Args:
        df: DataFrame to process
        columns: List of column names containing JSON strings
    Returns:
        DataFrame with parsed JSON columns
    """
    def safe_json_loads(x):
        if pd.isna(x):
            return []
        if isinstance(x, list):
            return x  # Already parsed
        try:
            # First attempt: standard JSON parsing
            return json.loads(x)
        except json.JSONDecodeError:
            try:
                # Second attempt: try fixing common JSON issues
                # Replace single quotes with double quotes if that's the issue
                fixed_json = x.replace("'", '"')
                return json.loads(fixed_json)
            except json.JSONDecodeError:
                try:
                    # Third attempt: try to use ast.literal_eval for Python literal structures
                    import ast
                    return ast.literal_eval(x)
                except (SyntaxError, ValueError):
                    try:
                        # Fourth attempt: handle potential trailing commas
                        # This is a common error in JSON strings
                        if x.endswith(',]}'):
                            fixed_json = x.replace(',]}', ']}')
                            return json.loads(fixed_json)
                        elif x.endswith(',}'):
                            fixed_json = x.replace(',}', '}')
                            return json.loads(fixed_json)
                        else:
                            # Create more detailed error reporting
                            print(f"Warning: Could not parse JSON: {x[:100]}...")
                            print(f"Using a fallback empty list")
                            return []
                    except json.JSONDecodeError as e:
                        print(f"Warning: Could not parse JSON: {x[:100]}...")
                        print(f"Error: {e}")
                        return []
    # Apply the safe parsing to each specified column
    for col in columns:
        # Check if column exists
        if col in df.columns:
            df[col] = df[col].apply(safe_json_loads)
        else:
            print(f"Warning: Column '{col}' not found in DataFrame")
    return df
def extract_data(input_dir: str, movies_file: str, credits_file: str) -> pd.DataFrame:
    """
    Extract data from raw CSV files and prepare it for transformation.
    This function handles the following tasks:
    1. Reads movie and credits data from CSV files
    2. Resolves column naming conflicts during the merge (e.g., 'title' appearing in both files)
    3. Parses JSON-formatted columns into Python objects
    4. Adds default values for any missing required columns
    Implementation details:
    - Both the movies and credits files contain a 'title' column, which causes pandas to append
      '_x' and '_y' suffixes during merging. We handle this by creating a new 'title' column from
      'title_x' and dropping both suffixed columns.
    - JSON parsing is improved to handle various edge cases and problematic JSON formatting.
    - Validation is performed to ensure all required columns are present before transformation.
    Args:
        input_dir: Directory containing input files
        movies_file: Filename for movies data
        credits_file: Filename for credits data
    Returns:
        DataFrame containing merged movie data ready for transformation
    """
    print("Extracting data...")
    # Check if files exist
    movies_path = os.path.join(input_dir, movies_file)
    credits_path = os.path.join(input_dir, credits_file)
    if not os.path.exists(movies_path):
        raise FileNotFoundError(f"Movies file not found: {movies_path}")
    if not os.path.exists(credits_path):
        raise FileNotFoundError(f"Credits file not found: {credits_path}")
    print(f"Reading movies data from {movies_path}")
    movies_df = pd.read_csv(movies_path)
    print(f"Movies data shape: {movies_df.shape}")
    print(f"Movies columns: {movies_df.columns.tolist()}")
    print(f"Reading credits data from {credits_path}")
    credits_df = pd.read_csv(credits_path)
    print(f"Credits data shape: {credits_df.shape}")
    print(f"Credits columns: {credits_df.columns.tolist()}")
    # Handle different column naming conventions
    # Some datasets use 'movie_id', some use 'id' in the credits file
    id_column_in_movies = 'id'
    id_column_in_credits = None
    # Determine the ID column in credits
    if 'movie_id' in credits_df.columns:
        id_column_in_credits = 'movie_id'
    elif 'id' in credits_df.columns:
        id_column_in_credits = 'id'
    else:
        raise ValueError("Could not find 'id' or 'movie_id' column in credits file")
    # Rename ID column in credits to match movies
    if id_column_in_credits != id_column_in_movies:
        print(f"Renaming column '{id_column_in_credits}' to '{id_column_in_movies}' in credits data")
        credits_df.rename(columns={id_column_in_credits: id_column_in_movies}, inplace=True)
    # Both dataframes have a 'title' column which will cause duplicates when merging
    # We'll handle this by tracking which columns might get suffixes
    common_columns = set(movies_df.columns) & set(credits_df.columns)
    print(f"Common columns that may get suffixes: {common_columns}")
    # Merge dataframes
    print(f"Merging data on column '{id_column_in_movies}'")
    df = pd.merge(movies_df, credits_df, on=id_column_in_movies)
    print(f"Merged data shape: {df.shape}")
    # Check and handle duplicate columns with suffixes
    duplicate_cols = [col for col in df.columns if '_x' in col or '_y' in col]
    if duplicate_cols:
        print(f"Found duplicate columns after merge: {duplicate_cols}")
        # If title has been split into title_x and title_y, fix it
        if 'title_x' in df.columns and 'title_y' in df.columns:
            print("Fixing title column (using title_x from movies file)")
            # Rename title_x to title (preserve the title from the movies file)
            df['title'] = df['title_x']
            # Drop the duplicate columns
            df = df.drop(columns=['title_x', 'title_y'])
    # Identify JSON columns
    json_columns = []
    for col in ['genres', 'keywords', 'production_companies', 'cast', 'crew']:
        if col in df.columns:
            json_columns.append(col)
        else:
            print(f"Warning: Expected column '{col}' not found in merged dataframe")
    print(f"Parsing JSON in columns: {json_columns}")
    # Convert string representations of lists/dicts to actual Python objects
    df = parse_json_fields(df, json_columns)
    # Verify that required columns are present for transformation
    required_columns = ['id', 'title', 'release_date', 'budget', 'revenue', 
                        'popularity', 'vote_average', 'vote_count', 'overview']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Warning: Missing required columns: {missing_columns}")
        # Add missing columns with default values to prevent KeyError later
        for col in missing_columns:
            print(f"Adding missing column '{col}' with default values")
            if col == 'title':
                df[col] = 'Unknown Title'
            elif col in ['release_date', 'overview']:
                df[col] = ''
            elif col in ['budget', 'revenue', 'popularity', 'vote_average', 'vote_count']:
                df[col] = 0
            else:
                df[col] = None
    return df
def transform_data(df: pd.DataFrame) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """
    Transform the data into Neo4j compatible format.
    Args:
        df: DataFrame containing movie data
    Returns:
        Dictionary containing nodes and relationships data
    """
    print("Transforming data...")
    # Extract nodes
    movies = []
    persons = {}  # Using dict to avoid duplicates
    genres = {}   # Using dict to avoid duplicates
    keywords = {}  # Using dict to avoid duplicates
    companies = {}  # Using dict to avoid duplicates
    # Extract relationships
    acted_in = []
    directed = []
    produced = []
    categorized_as = []
    tagged_with = []
    # Process each movie
    for _, row in df.iterrows():
        movie_id = row['id']
        # Movie node
        movie = {
            'id': movie_id,
            'title': row['title'],
            'release_date': row['release_date'],
            'budget': row['budget'],
            'revenue': row['revenue'],
            'popularity': row['popularity'],
            'vote_average': row['vote_average'],
            'vote_count': row['vote_count'],
            'overview': row['overview']
        }
        movies.append(movie)
        # Process genres
        for genre in row['genres']:
            genre_id = genre['id']
            genres[genre_id] = {'id': genre_id, 'name': genre['name']}
            categorized_as.append({'movie_id': movie_id, 'genre_id': genre_id})
        # Process keywords
        for keyword in row['keywords']:
            keyword_id = keyword['id']
            keywords[keyword_id] = {'id': keyword_id, 'name': keyword['name']}
            tagged_with.append({'movie_id': movie_id, 'keyword_id': keyword_id})
        # Process production companies
        for company in row['production_companies']:
            company_id = company['id']
            companies[company_id] = {
                'id': company_id, 
                'name': company['name'],
                'origin_country': company.get('origin_country', '')
            }
            produced.append({'movie_id': movie_id, 'company_id': company_id})
        # Process cast (actors)
        for i, actor in enumerate(row['cast'][:10]):  # Limit to top 10 actors
            person_id = actor['id']
            persons[person_id] = {
                'id': person_id,
                'name': actor['name'],
                'gender': actor.get('gender', 0),
                'profile_path': actor.get('profile_path', '')
            }
            acted_in.append({
                'person_id': person_id,
                'movie_id': movie_id,
                'character': actor['character'],
                'order': i
            })
        # Process crew (focus on directors)
        for crew_member in row['crew']:
            if crew_member['job'] == 'Director':
                person_id = crew_member['id']
                persons[person_id] = {
                    'id': person_id,
                    'name': crew_member['name'],
                    'gender': crew_member.get('gender', 0),
                    'profile_path': crew_member.get('profile_path', '')
                }
                directed.append({
                    'person_id': person_id,
                    'movie_id': movie_id,
                    'job': crew_member['job'],
                    'department': crew_member['department']
                })
    # Convert dictionaries to lists
    genres_list = list(genres.values())
    keywords_list = list(keywords.values())
    companies_list = list(companies.values())
    persons_list = list(persons.values())
    transformed_data = {
        'nodes': {
            'movies': movies,
            'persons': persons_list,
            'genres': genres_list,
            'keywords': keywords_list,
            'companies': companies_list
        },
        'relationships': {
            'acted_in': acted_in,
            'directed': directed,
            'produced': produced,
            'categorized_as': categorized_as,
            'tagged_with': tagged_with
        }
    }
    return transformed_data
def load_data(data: Dict[str, Dict[str, List[Dict[str, Any]]]], output_dir: str) -> None:
    """
    Save transformed data as CSV files for Neo4j import.
    Args:
        data: Dictionary containing transformed data
        output_dir: Directory to save output files
    """
    print("Loading data...")
    ensure_dir(output_dir)
    # Save nodes
    print("Saving nodes...")
    pd.DataFrame(data['nodes']['movies']).to_csv(os.path.join(output_dir, 'movies.csv'), index=False)
    pd.DataFrame(data['nodes']['persons']).to_csv(os.path.join(output_dir, 'persons.csv'), index=False)
    pd.DataFrame(data['nodes']['genres']).to_csv(os.path.join(output_dir, 'genres.csv'), index=False)
    pd.DataFrame(data['nodes']['keywords']).to_csv(os.path.join(output_dir, 'keywords.csv'), index=False)
    pd.DataFrame(data['nodes']['companies']).to_csv(os.path.join(output_dir, 'companies.csv'), index=False)
    # Save relationships
    print("Saving relationships...")
    pd.DataFrame(data['relationships']['acted_in']).to_csv(os.path.join(output_dir, 'acted_in.csv'), index=False)
    pd.DataFrame(data['relationships']['directed']).to_csv(os.path.join(output_dir, 'directed.csv'), index=False)
    pd.DataFrame(data['relationships']['produced']).to_csv(os.path.join(output_dir, 'produced.csv'), index=False)
    pd.DataFrame(data['relationships']['categorized_as']).to_csv(os.path.join(output_dir, 'categorized_as.csv'), index=False)
    pd.DataFrame(data['relationships']['tagged_with']).to_csv(os.path.join(output_dir, 'tagged_with.csv'), index=False)
def run_etl(input_dir: str, output_dir: str, movies_file: str = "tmdb_5000_movies.csv",
            credits_file: str = "tmdb_5000_credits.csv") -> None:
    """
    Run the complete ETL process.
    Args:
        input_dir: Directory containing input files
        output_dir: Directory to save output files
        movies_file: Filename for movies data
        credits_file: Filename for credits data
    """
    print("Starting ETL process...")
    # Extract
    data_df = extract_data(input_dir, movies_file, credits_file)
    # Transform
    transformed_data = transform_data(data_df)
    # Load
    load_data(transformed_data, output_dir)
    print("ETL process completed successfully!")
</file>

<file path="movie_graph/etl/README.md">
# ETL Process for Neo4j Movie Analysis

This module handles the Extract, Transform, Load (ETL) process for preparing TMDB movie data for import into Neo4j.

## Overview

The ETL process consists of three main steps:

1. **Extract**: Reading raw data from CSV files
2. **Transform**: Converting the data into a Neo4j-friendly format
3. **Load**: Saving the transformed data as CSV files for Neo4j import

## Data Requirements

The process requires two primary data files:

- `tmdb_5000_movies.csv`: Contains movie details including title, budget, revenue, etc.
- `tmdb_5000_credits.csv`: Contains cast and crew information for each movie

These files should be placed in the `data/raw` directory before running the process.

## Running the ETL Process

To run the ETL process:

```bash
python scripts/etl_process.py --input data/raw --output data/processed
```

## Improvements and Fixes

### JSON Parsing

The ETL process includes robust JSON parsing that handles common issues:

- Single vs. double quotes in JSON strings
- Trailing commas in JSON arrays/objects
- Malformed JSON structures
- Empty/null values

The `parse_json_fields` function uses a multi-stage approach to process JSON data:
1. Standard JSON parsing
2. Common error fixes (like replacing single quotes)
3. Using `ast.literal_eval` for Python literal structures
4. Handling specific patterns like trailing commas

### Column Handling

When merging dataframes with duplicate column names (e.g., 'title' appears in both movies and credits files), the process:

1. Identifies columns that might get suffixes during merge
2. After merge, creates a new column with the desired name (e.g., new 'title' from 'title_x')
3. Drops the suffixed columns to clean up the resulting DataFrame

### Missing Column Handling

The process checks for required columns and provides default values when needed:

- For 'title': "Unknown Title"
- For text fields like 'release_date' and 'overview': Empty string
- For numeric fields: 0
- For other fields: None

## Output Files

The process creates the following output files in the specified output directory:

### Node Files
- `movies.csv`: Movie nodes with details
- `persons.csv`: Person nodes (actors, directors)
- `genres.csv`: Genre nodes
- `keywords.csv`: Keyword nodes
- `companies.csv`: Production company nodes

### Relationship Files
- `acted_in.csv`: Relationships between persons and movies (acting)
- `directed.csv`: Relationships between persons and movies (directing)
- `produced.csv`: Relationships between companies and movies
- `categorized_as.csv`: Relationships between movies and genres
- `tagged_with.csv`: Relationships between movies and keywords

## Troubleshooting

If you encounter issues with the ETL process:

1. **Data Files Missing**: Ensure the required CSV files are in the `data/raw` directory
2. **JSON Parsing Errors**: Check if your data has non-standard JSON that might need additional parsing rules
3. **Missing Columns**: Verify that your data contains all the expected columns
4. **Duplicate Columns**: The process handles duplicate column names, but you may need to adjust if your data has different naming patterns

For detailed debugging, you can use the `scripts/debug_extract.py` script to analyze your data structure.
</file>

<file path="movie_graph/utils/helpers.py">
"""
Utility helper functions for the Neo4j Movie Analysis project.
"""
import json
import re
import os
from typing import Dict, List, Any, Union, Optional
def ensure_dir(directory: str) -> None:
    """
    Create directory if it doesn't exist.
    Args:
        directory: Path to directory to create
    """
    if not os.path.exists(directory):
        os.makedirs(directory)
def convert_text_to_notebook(input_file: str, output_file: str) -> None:
    """
    Convert text file to Jupyter notebook format.
    Args:
        input_file: Path to input text file
        output_file: Path to output notebook file
    """
    # Ensure output directory exists
    ensure_dir(os.path.dirname(output_file))
    # Read the text file
    with open(input_file, 'r') as f:
        content = f.read()
    # Initialize notebook structure
    notebook = {
        'cells': [],
        'metadata': {
            'kernelspec': {
                'display_name': 'Python 3',
                'language': 'python',
                'name': 'python3'
            },
            'language_info': {
                'codemirror_mode': {
                    'name': 'ipython',
                    'version': 3
                },
                'file_extension': '.py',
                'mimetype': 'text/x-python',
                'name': 'python',
                'nbconvert_exporter': 'python',
                'pygments_lexer': 'ipython3',
                'version': '3.8.0'
            }
        },
        'nbformat': 4,
        'nbformat_minor': 4
    }
    # Split content into cells
    cell_pattern = re.compile(r'# %% \[(markdown|code)\](?: id=\"([^\"]+)\")?\n((?:.+\n)*?)(?=# %% |$)', re.DOTALL)
    matches = cell_pattern.finditer(content)
    for match in matches:
        cell_type, cell_id, cell_content = match.groups()
        # Process cell content based on type
        if cell_type == 'markdown':
            # Remove the leading # from each line in markdown cells
            source = [line[2:] + '\n' if line.startswith('# ') else line + '\n' 
                     for line in cell_content.split('\n') if line]
            cell = {
                'cell_type': 'markdown',
                'metadata': {'id': cell_id} if cell_id else {},
                'source': source
            }
        else:  # code cell
            source = [line + '\n' for line in cell_content.split('\n') if line]
            cell = {
                'cell_type': 'code',
                'execution_count': None,
                'metadata': {'id': cell_id} if cell_id else {},
                'outputs': [],
                'source': source
            }
        notebook['cells'].append(cell)
    # Write to ipynb file
    with open(output_file, 'w') as f:
        json.dump(notebook, f, indent=2)
    print(f'Conversion completed: {output_file} created successfully.')
def clean_text(text: str) -> str:
    """
    Clean and normalize text for consistency.
    Args:
        text: Text to clean
    Returns:
        Cleaned text
    """
    if not text:
        return ""
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text.strip())
    return text
</file>

<file path="movie_graph/__init__.py">
"""
Neo4j Movie Analysis package.
This package provides tools for analyzing movie data with Neo4j.
"""
from typing import Tuple, Optional
from movie_graph.db.connection import Neo4jConnection, get_connection
from movie_graph.etl.process import run_etl
__version__ = "0.1.0"
def connect_to_neo4j(uri: str, auth: Tuple[str, str], database: Optional[str] = None) -> Neo4jConnection:
    """
    Connect to Neo4j database.
    A convenience function to easily connect to Neo4j from anywhere.
    Args:
        uri: Neo4j connection URI
        auth: Tuple containing (username, password)
        database: Optional database name
    Returns:
        A connected Neo4jConnection instance
    """
    return get_connection(uri, auth, database)
</file>

<file path="notebooks/cypher/analysis_queries.cypher">
// Neo4j Movie Graph Analysis Queries

// 1. Basic movie information retrieval
// Get details of a specific movie
MATCH (m:Movie {title: 'The Dark Knight'})
RETURN m;

// 2. Connected entity retrieval
// Find all actors who appeared in a specific movie
MATCH (p:Person)-[r:ACTED_IN]->(m:Movie {title: 'The Dark Knight'})
RETURN p.name, r.character
ORDER BY r.order;

// Find directors of a movie
MATCH (p:Person)-[:DIRECTED]->(m:Movie {title: 'The Dark Knight'})
RETURN p.name;

// Find all genres of a movie
MATCH (m:Movie {title: 'The Dark Knight'})-[:CATEGORIZED_AS]->(g:Genre)
RETURN g.name;

// 3. Entity relationship insights
// Movies with the most actors
MATCH (p:Person)-[:ACTED_IN]->(m:Movie)
WITH m, count(p) AS actor_count
RETURN m.title, actor_count
ORDER BY actor_count DESC
LIMIT 10;

// Actors who worked in the most movies
MATCH (p:Person)-[:ACTED_IN]->(m:Movie)
WITH p, count(m) AS movie_count
RETURN p.name, movie_count
ORDER BY movie_count DESC
LIMIT 10;

// Most common genres
MATCH (m:Movie)-[:CATEGORIZED_AS]->(g:Genre)
WITH g, count(m) AS movie_count
RETURN g.name, movie_count
ORDER BY movie_count DESC;

// 4. Path traversal and connections
// Find how actors are connected through movies (3 levels deep)
MATCH path = (p1:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(:Person)-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(p2:Person)
WHERE p1 <> p2
RETURN p2.name, length(path)
ORDER BY length(path)
LIMIT 10;

// 5. Recommendations
// Movie recommendations based on genre similarity
MATCH (m:Movie {title: 'The Dark Knight'})-[:CATEGORIZED_AS]->(g:Genre)<-[:CATEGORIZED_AS]-(rec:Movie)
WHERE m <> rec
WITH rec, collect(g.name) AS genres, count(g) AS genre_overlap
WHERE genre_overlap > 2
RETURN rec.title, rec.vote_average, genres, genre_overlap
ORDER BY genre_overlap DESC, rec.vote_average DESC
LIMIT 10;

// Movie recommendations based on cast overlap
MATCH (m:Movie {title: 'The Dark Knight'})<-[:ACTED_IN]-(a:Person)-[:ACTED_IN]->(rec:Movie)
WHERE m <> rec
WITH rec, collect(a.name) AS shared_actors, count(a) AS actor_count
WHERE actor_count > 1
RETURN rec.title, rec.vote_average, shared_actors, actor_count
ORDER BY actor_count DESC, rec.vote_average DESC
LIMIT 10;

// 6. Advanced pattern matching
// Find movies with the same director and lead actor
MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person)
MATCH (d)-[:DIRECTED]->(m2:Movie)<-[:ACTED_IN]-(a)
WHERE m <> m2 AND a.name <> d.name
RETURN d.name AS director, a.name AS actor, m.title AS movie1, m2.title AS movie2;

// 7. Aggregations and analysis
// Average rating by genre
MATCH (m:Movie)-[:CATEGORIZED_AS]->(g:Genre)
WITH g, avg(m.vote_average) AS avg_rating, count(m) AS movie_count
WHERE movie_count > 5
RETURN g.name, avg_rating, movie_count
ORDER BY avg_rating DESC;

// Production companies by movie count
MATCH (c:Company)-[:PRODUCED]->(m:Movie)
WITH c, count(m) AS movie_count
RETURN c.name, movie_count
ORDER BY movie_count DESC
LIMIT 20;

// 8. Temporal analysis
// Movie count by year
MATCH (m:Movie)
WHERE m.release_date IS NOT NULL
WITH m.release_date.year AS year, count(m) AS movie_count
RETURN year, movie_count
ORDER BY year;

// 9. Cast and crew analysis
// Directors who worked with the same actor multiple times
MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person)
WITH d, a, count(m) AS collaboration_count
WHERE collaboration_count > 1
RETURN d.name AS director, a.name AS actor, collaboration_count
ORDER BY collaboration_count DESC
LIMIT 20;

// 10. Advanced recommendations with similarity metrics
// Find similar movies using multiple criteria with weighted scoring
MATCH (m:Movie {title: 'Inception'})
MATCH (other:Movie) WHERE m <> other

// Genre similarity
OPTIONAL MATCH (m)-[:CATEGORIZED_AS]->(g:Genre)<-[:CATEGORIZED_AS]-(other)
WITH m, other, count(g) AS genre_overlap

// Director similarity
OPTIONAL MATCH (d:Person)-[:DIRECTED]->(m), (d)-[:DIRECTED]->(other)
WITH m, other, genre_overlap, count(d) AS same_director

// Cast similarity  
OPTIONAL MATCH (a:Person)-[:ACTED_IN]->(m), (a)-[:ACTED_IN]->(other)
WITH m, other, genre_overlap, same_director, count(a) AS cast_overlap

// Calculate similarity score with weights
WITH other, 
     (genre_overlap * 3) + 
     (same_director * 5) + 
     (cast_overlap * 2) AS similarity_score
WHERE similarity_score > 0

RETURN other.title, similarity_score
ORDER BY similarity_score DESC
LIMIT 15;
</file>

<file path="notebooks/cypher/import_data.cypher">
// Neo4j Movie Graph Import Script

// Reset database (optional - use carefully in production)
MATCH (n) DETACH DELETE n;

// Create constraints for better performance
CREATE CONSTRAINT movie_id IF NOT EXISTS FOR (m:Movie) REQUIRE m.id IS UNIQUE;
CREATE CONSTRAINT person_id IF NOT EXISTS FOR (p:Person) REQUIRE p.id IS UNIQUE;
CREATE CONSTRAINT genre_id IF NOT EXISTS FOR (g:Genre) REQUIRE g.id IS UNIQUE;
CREATE CONSTRAINT keyword_id IF NOT EXISTS FOR (k:Keyword) REQUIRE k.id IS UNIQUE;
CREATE CONSTRAINT company_id IF NOT EXISTS FOR (c:Company) REQUIRE c.id IS UNIQUE;

// Import Movie nodes
LOAD CSV WITH HEADERS FROM 'file:///movies.csv' AS row
CREATE (m:Movie {
  id: toInteger(row.id),
  title: row.title,
  release_date: date(row.release_date),
  budget: toInteger(row.budget),
  revenue: toInteger(row.revenue),
  popularity: toFloat(row.popularity),
  vote_average: toFloat(row.vote_average),
  vote_count: toInteger(row.vote_count),
  overview: row.overview
});

// Import Person nodes
LOAD CSV WITH HEADERS FROM 'file:///persons.csv' AS row
CREATE (p:Person {
  id: toInteger(row.id),
  name: row.name,
  gender: toInteger(row.gender),
  profile_path: row.profile_path
});

// Import Genre nodes
LOAD CSV WITH HEADERS FROM 'file:///genres.csv' AS row
CREATE (g:Genre {
  id: toInteger(row.id),
  name: row.name
});

// Import Keyword nodes
LOAD CSV WITH HEADERS FROM 'file:///keywords.csv' AS row
CREATE (k:Keyword {
  id: toInteger(row.id),
  name: row.name
});

// Import Company nodes
LOAD CSV WITH HEADERS FROM 'file:///companies.csv' AS row
CREATE (c:Company {
  id: toInteger(row.id),
  name: row.name,
  origin_country: row.origin_country
});

// Import ACTED_IN relationships
LOAD CSV WITH HEADERS FROM 'file:///acted_in.csv' AS row
MATCH (p:Person {id: toInteger(row.person_id)})
MATCH (m:Movie {id: toInteger(row.movie_id)})
CREATE (p)-[:ACTED_IN {character: row.character, order: toInteger(row.order)}]->(m);

// Import DIRECTED relationships
LOAD CSV WITH HEADERS FROM 'file:///directed.csv' AS row
MATCH (p:Person {id: toInteger(row.person_id)})
MATCH (m:Movie {id: toInteger(row.movie_id)})
CREATE (p)-[:DIRECTED {job: row.job, department: row.department}]->(m);

// Import PRODUCED relationships
LOAD CSV WITH HEADERS FROM 'file:///produced.csv' AS row
MATCH (c:Company {id: toInteger(row.company_id)})
MATCH (m:Movie {id: toInteger(row.movie_id)})
CREATE (c)-[:PRODUCED]->(m);

// Import CATEGORIZED_AS relationships
LOAD CSV WITH HEADERS FROM 'file:///categorized_as.csv' AS row
MATCH (m:Movie {id: toInteger(row.movie_id)})
MATCH (g:Genre {id: toInteger(row.genre_id)})
CREATE (m)-[:CATEGORIZED_AS]->(g);

// Import TAGGED_WITH relationships
LOAD CSV WITH HEADERS FROM 'file:///tagged_with.csv' AS row
MATCH (m:Movie {id: toInteger(row.movie_id)})
MATCH (k:Keyword {id: toInteger(row.keyword_id)})
CREATE (m)-[:TAGGED_WITH]->(k);

// Create derived SIMILAR_TO relationships based on genre overlap
MATCH (m1:Movie)-[:CATEGORIZED_AS]->(g:Genre)<-[:CATEGORIZED_AS]-(m2:Movie)
WHERE m1 <> m2
WITH m1, m2, count(g) AS weight
WHERE weight > 2
MERGE (m1)-[r:SIMILAR_TO]-(m2)
ON CREATE SET r.similarity_score = weight;

// Create derived WORKED_WITH relationships between actors who appeared in the same movie
MATCH (p1:Person)-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(p2:Person)
WHERE p1 <> p2
WITH p1, p2, count(*) AS movie_count
WHERE movie_count > 1
MERGE (p1)-[r:WORKED_WITH]-(p2)
ON CREATE SET r.movie_count = movie_count;

// Create indexes for commonly searched properties
CREATE INDEX movie_title IF NOT EXISTS FOR (m:Movie) ON (m.title);
CREATE INDEX person_name IF NOT EXISTS FOR (p:Person) ON (p.name);
CREATE INDEX genre_name IF NOT EXISTS FOR (g:Genre) ON (g.name);
CREATE INDEX keyword_name IF NOT EXISTS FOR (k:Keyword) ON (k.name);
CREATE INDEX company_name IF NOT EXISTS FOR (c:Company) ON (c.name);
</file>

<file path="notebooks/movie_analysis_updated.ipynb">

</file>

<file path="notebooks/movie_analysis.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41d56e5",
   "metadata": {
    "id": "97f39187-b5d1-45c2-9012-1c8f9c5a86d4"
   },
   "source": [
    "# Movie Graph Analysis Demo\n",
    "\n",
    "This notebook demonstrates basic analysis of the Neo4j movie graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eed283b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T15:07:32.360164Z",
     "start_time": "2025-04-08T15:07:32.002878Z"
    },
    "id": "bb407747-66b7-45ec-86fa-d543e9b57d2d"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GraphDatabvase' from 'neo4j' (/Users/bprzybysz/nc-src/nordcloud-interviews/.venv/lib/python3.11/site-packages/neo4j/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneo4j\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphDatabvase\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Configure visualization\u001b[39;00m\n\u001b[32m      8\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mggplot\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'GraphDatabvase' from 'neo4j' (/Users/bprzybysz/nc-src/nordcloud-interviews/.venv/lib/python3.11/site-packages/neo4j/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from neo4j import GraphDatabvase\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afcbd7",
   "metadata": {
    "id": "16bd286e-ccee-46ae-af77-df3a6eba3982"
   },
   "source": [
    "## Database Connection\n",
    "\n",
    "Initialize the Neo4j connection using our standardized connection class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee692720",
   "metadata": {
    "id": "1a044638-8296-47f5-a646-d4c7f7fac927"
   },
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \"\"\"Handles Neo4j database connections and queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, auth: tuple):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=auth)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close the Neo4j connection.\"\"\"\n",
    "        self.driver.close()\n",
    "        \n",
    "    def query(self, query: str, parameters: dict = None) -> pd.DataFrame:\n",
    "        \"\"\"Execute Neo4j query and return results as DataFrame.\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, parameters)\n",
    "            return pd.DataFrame([r.values() for r in result], columns=result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51179515",
   "metadata": {
    "id": "e0374643-b26c-4e1f-86bc-88eb6b5311c6"
   },
   "outputs": [],
   "source": [
    "# Initialize connection\n",
    "URI = \"bolt://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"password\")  # Update with actual credentials\n",
    "\n",
    "try:\n",
    "    conn = Neo4jConnection(URI, AUTH)\n",
    "    print(\"Successfully connected to Neo4j\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="notebooks/neo4j_template.ipynb">
{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5a27e40-9a9c-47db-90a2-9c7bff882de6"
   },
   "source": [
    "# Neo4j Graph Analysis\n",
    "\n",
    "This notebook was created following the standard format for compatibility with Cursor and Spark environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "be8c3926-a0b9-45a5-895e-97bd070d1241"
   },
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2a1b67e-94ae-42c4-b422-052b9e017f74"
   },
   "source": [
    "## Spark Integration\n",
    "\n",
    "Initialize Spark session for data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5e1d2b9c-b44a-431b-bdd3-53d8c382fe5a"
   },
   "source": [
    "%%pyspark\n",
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Notebook\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "11ee5751-6c29-477b-a4be-64e241910d1c"
   },
   "source": [
    "# Load data with Spark\n",
    "# Replace with your actual data source\n",
    "df = spark.read.csv(\"data/sample.csv\", header=True, inferSchema=True)\n",
    "df = df.cache()  # Improved performance with caching\n",
    "\n",
    "# Display sample data\n",
    "df.limit(5).toPandas()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5d4f4b8c-642b-44bf-8822-fbf453916942"
   },
   "source": [
    "%%pyspark\n",
    "# Spark SQL example\n",
    "df.createOrReplaceTempView(\"data\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM data\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "result.toPandas()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aac5b1e5-31dd-4352-b17b-1f462d97a2c9"
   },
   "source": [
    "## Data Analysis\n",
    "\n",
    "This section contains data analysis code.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1c147422-3326-40a8-8e34-15c81425c462"
   },
   "source": [
    "# Data analysis example\n",
    "# Replace with your actual analysis code\n",
    "\n",
    "# Sample DataFrame for demonstration\n",
    "sample_df = pd.DataFrame({\n",
    "    'A': np.random.randn(100),\n",
    "    'B': np.random.randn(100),\n",
    "    'C': np.random.randn(100)\n",
    "})\n",
    "\n",
    "# Display sample data\n",
    "sample_df.head()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "16e8bc8b-24c0-493a-b0c6-6f88045f6966"
   },
   "source": [
    "# Visualization example\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=sample_df, x='A', kde=True)\n",
    "plt.title('Distribution of A')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a17ea9e2-e34c-4304-bab4-5e9067db02c7"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Summary of findings and next steps.\n"
   ]
  }
 ]
}
</file>

<file path="scripts/convert_notebook.py">
#!/usr/bin/env python3
"""
Script to convert text files to Jupyter notebook format.
"""
import os
import sys
import argparse
# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from movie_graph.utils.helpers import convert_text_to_notebook
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Convert text file to Jupyter notebook')
    parser.add_argument('input', type=str, help='Input text file path')
    parser.add_argument('output', type=str, help='Output notebook file path')
    return parser.parse_args()
def main():
    """Main conversion process."""
    args = parse_args()
    # Convert text to notebook
    convert_text_to_notebook(args.input, args.output)
if __name__ == "__main__":
    main()
</file>

<file path="scripts/create_notebook_template.py">
#!/usr/bin/env python
"""
Jupyter Notebook Template Generator
This script creates a properly formatted Jupyter notebook template
that adheres to the Cursor and Spark compatibility requirements.
"""
import json
import os
import sys
import uuid
from pathlib import Path
from typing import Dict, Any, Optional
import nbformat
from nbformat.validator import validate
def create_notebook_template(
    output_path: str,
    title: str = "Notebook Template",
    with_spark: bool = True
) -> bool:
    """
    Creates a properly formatted Jupyter notebook template.
    Args:
        output_path: Path to save the notebook
        title: Title of the notebook
        with_spark: Whether to include Spark integration
    Returns:
        Success status
    """
    # Create notebook structure
    notebook = {
        "nbformat": 4,
        "nbformat_minor": 5,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.11.8"
            }
        },
        "cells": []
    }
    # Add title cell
    title_cell = {
        "cell_type": "markdown",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            f"# {title}\n",
            "\n",
            "This notebook was created following the standard format for compatibility with Cursor and Spark environments.\n"
        ]
    }
    notebook["cells"].append(title_cell)
    # Add imports cell
    imports_cell = {
        "cell_type": "code",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            "# Standard imports\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import seaborn as sns\n",
            "\n",
            "# Configure visualization\n",
            "plt.style.use('ggplot')\n",
            "sns.set(style=\"whitegrid\")\n"
        ],
        "execution_count": None,
        "outputs": []
    }
    notebook["cells"].append(imports_cell)
    # Add Spark session initialization if requested
    if with_spark:
        spark_section_cell = {
            "cell_type": "markdown",
            "metadata": {"id": str(uuid.uuid4())},
            "source": [
                "## Spark Integration\n",
                "\n",
                "Initialize Spark session for data processing.\n"
            ]
        }
        notebook["cells"].append(spark_section_cell)
        spark_init_cell = {
            "cell_type": "code",
            "metadata": {"id": str(uuid.uuid4())},
            "source": [
                "# Initialize Spark Session\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"Notebook\") \\\n",
                "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
                "    .getOrCreate()\n"
            ],
            "execution_count": None,
            "outputs": []
        }
        notebook["cells"].append(spark_init_cell)
        spark_data_cell = {
            "cell_type": "code",
            "metadata": {"id": str(uuid.uuid4())},
            "source": [
                "# Load data with Spark\n",
                "# Replace with your actual data source\n",
                "df = spark.read.csv(\"data/sample.csv\", header=True, inferSchema=True)\n",
                "df = df.cache()  # Improved performance with caching\n",
                "\n",
                "# Display sample data\n",
                "df.limit(5).toPandas()\n"
            ],
            "execution_count": None,
            "outputs": []
        }
        notebook["cells"].append(spark_data_cell)
        spark_sql_cell = {
            "cell_type": "code",
            "metadata": {"id": str(uuid.uuid4())},
            "source": [
                "%%pyspark\n",
                "# Spark SQL example\n",
                "df.createOrReplaceTempView(\"data\")\n",
                "\n",
                "result = spark.sql(\"\"\"\n",
                "SELECT *\n",
                "FROM data\n",
                "LIMIT 10\n",
                "\"\"\")\n",
                "\n",
                "result.toPandas()\n"
            ],
            "execution_count": None,
            "outputs": []
        }
        notebook["cells"].append(spark_sql_cell)
    # Add data analysis section
    analysis_section_cell = {
        "cell_type": "markdown",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            "## Data Analysis\n",
            "\n",
            "This section contains data analysis code.\n"
        ]
    }
    notebook["cells"].append(analysis_section_cell)
    analysis_code_cell = {
        "cell_type": "code",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            "# Data analysis example\n",
            "# Replace with your actual analysis code\n",
            "\n",
            "# Sample DataFrame for demonstration\n",
            "sample_df = pd.DataFrame({\n",
            "    'A': np.random.randn(100),\n",
            "    'B': np.random.randn(100),\n",
            "    'C': np.random.randn(100)\n",
            "})\n",
            "\n",
            "# Display sample data\n",
            "sample_df.head()\n"
        ],
        "execution_count": None,
        "outputs": []
    }
    notebook["cells"].append(analysis_code_cell)
    visualization_code_cell = {
        "cell_type": "code",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            "# Visualization example\n",
            "plt.figure(figsize=(10, 6))\n",
            "sns.histplot(data=sample_df, x='A', kde=True)\n",
            "plt.title('Distribution of A')\n",
            "plt.xlabel('Value')\n",
            "plt.ylabel('Frequency')\n",
            "plt.grid(True)\n",
            "plt.show()\n"
        ],
        "execution_count": None,
        "outputs": []
    }
    notebook["cells"].append(visualization_code_cell)
    # Add conclusion section
    conclusion_cell = {
        "cell_type": "markdown",
        "metadata": {"id": str(uuid.uuid4())},
        "source": [
            "## Conclusion\n",
            "\n",
            "Summary of findings and next steps.\n"
        ]
    }
    notebook["cells"].append(conclusion_cell)
    # Save notebook
    try:
        # Validate with nbformat
        nb = nbformat.reads(json.dumps(notebook), as_version=4)
        validate(nb)
        # Ensure the directory exists
        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
        # Write to file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(notebook, f, indent=1)
        print(f" Successfully created notebook template at {output_path}")
        return True
    except Exception as e:
        print(f" Error creating notebook template: {str(e)}")
        return False
def main():
    """Main function to create a notebook template."""
    if len(sys.argv) < 2:
        print("Usage: python create_notebook_template.py <output_path> [title] [with_spark]")
        print("  output_path: Path to save the notebook")
        print("  title: Title of the notebook (optional, defaults to 'Notebook Template')")
        print("  with_spark: Include Spark integration (optional, 'true' or 'false', defaults to 'true')")
        sys.exit(1)
    output_path = sys.argv[1]
    title = sys.argv[2] if len(sys.argv) > 2 else "Notebook Template"
    with_spark = True
    if len(sys.argv) > 3:
        with_spark = sys.argv[3].lower() in ('true', 'yes', '1')
    success = create_notebook_template(output_path, title, with_spark)
    sys.exit(0 if success else 1)
if __name__ == "__main__":
    main()
</file>

<file path="scripts/debug_extract.py">
#!/usr/bin/env python3
"""
Debugging script to understand column access issues in the ETL process.
"""
import os
import sys
import pandas as pd
# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from movie_graph.etl.process import parse_json_fields
def debug_extract():
    """Debug the extraction process."""
    input_dir = "data/raw"
    movies_file = "tmdb_5000_movies.csv"
    credits_file = "tmdb_5000_credits.csv"
    print("Reading movies data...")
    movies_path = os.path.join(input_dir, movies_file)
    movies_df = pd.read_csv(movies_path)
    print(f"Movies data shape: {movies_df.shape}")
    print(f"Movies columns: {movies_df.columns.tolist()}")
    # Print first 5 movie titles
    print("\nSample movie titles from movies_df:")
    if 'title' in movies_df.columns:
        print(movies_df['title'].head(5).tolist())
    else:
        print("'title' column not found in movies_df!")
    print("\nReading credits data...")
    credits_path = os.path.join(input_dir, credits_file)
    credits_df = pd.read_csv(credits_path)
    print(f"Credits data shape: {credits_df.shape}")
    print(f"Credits columns: {credits_df.columns.tolist()}")
    # Print first 5 movie titles from credits
    print("\nSample movie titles from credits_df:")
    if 'title' in credits_df.columns:
        print(credits_df['title'].head(5).tolist())
    else:
        print("'title' column not found in credits_df!")
    # Rename ID column in credits to match movies
    id_column_in_movies = 'id'
    id_column_in_credits = None
    if 'movie_id' in credits_df.columns:
        id_column_in_credits = 'movie_id'
    elif 'id' in credits_df.columns:
        id_column_in_credits = 'id'
    else:
        raise ValueError("Could not find 'id' or 'movie_id' column in credits file")
    if id_column_in_credits != id_column_in_movies:
        print(f"\nRenaming column '{id_column_in_credits}' to '{id_column_in_movies}' in credits data")
        credits_df.rename(columns={id_column_in_credits: id_column_in_movies}, inplace=True)
    # Merge dataframes
    print(f"\nMerging data on column '{id_column_in_movies}'")
    df = pd.merge(movies_df, credits_df, on=id_column_in_movies)
    print(f"Merged data shape: {df.shape}")
    print(f"Merged data columns: {df.columns.tolist()}")
    # Print column names with '_x' or '_y' suffix (indicates duplicates)
    duplicate_cols = [col for col in df.columns if '_x' in col or '_y' in col]
    if duplicate_cols:
        print(f"\nDuplicate columns after merge: {duplicate_cols}")
    # Check for title columns with suffix
    if 'title_x' in df.columns and 'title_y' in df.columns:
        print("\nFound title_x and title_y columns after merge. This indicates duplicate 'title' columns.")
        print("Sample values from title_x:")
        print(df['title_x'].head(5).tolist())
        print("Sample values from title_y:")
        print(df['title_y'].head(5).tolist())
    # Convert string representations of lists/dicts to actual Python objects
    json_columns = ['genres', 'keywords', 'production_companies', 'cast', 'crew']
    print(f"\nParsing JSON in columns: {json_columns}")
    df = parse_json_fields(df, json_columns)
    # Check if 'title' column exists after processing
    if 'title' in df.columns:
        print("\nTitle column exists after parsing JSON")
        print("Sample movie titles from merged df:")
        print(df['title'].head(5).tolist())
    else:
        print("\n'title' column NOT FOUND after parsing JSON!")
        # Try to find title with suffix
        if 'title_x' in df.columns:
            print("Found 'title_x' column instead:")
            print(df['title_x'].head(5).tolist())
        if 'title_y' in df.columns:
            print("Found 'title_y' column instead:")
            print(df['title_y'].head(5).tolist())
    return df
if __name__ == "__main__":
    df = debug_extract()
    # Save a small sample to CSV for inspection
    sample_df = df.head(10)
    sample_df.to_csv("data/sample_merged.csv", index=False)
    print("\nSaved 10 rows sample to data/sample_merged.csv for inspection")
</file>

<file path="scripts/download_data.py">
#!/usr/bin/env python3
"""
Script to download TMDB dataset from direct URLs.
"""
import os
import sys
import shutil
import urllib.request
from pathlib import Path
from zipfile import ZipFile
def download_tmdb_dataset():
    """Download TMDB dataset from direct URLs."""
    data_dir = Path("data/raw")
    data_dir.mkdir(parents=True, exist_ok=True)
    # Create a temporary directory for download
    temp_dir = Path("data/temp")
    temp_dir.mkdir(parents=True, exist_ok=True)
    # Define direct URLs for the dataset files
    urls = {
        'tmdb_5000_movies.csv': 'https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/tmdb_5000_movies.csv',
        'tmdb_5000_credits.csv': 'https://raw.githubusercontent.com/fivethirtyeight/data/master/fandango/tmdb_5000_credits.csv'
    }
    # Alternative URLs if the above fail
    backup_urls = {
        'tmdb_5000_movies.csv': 'https://github.com/bprzybysz/neo4j/files/11600275/tmdb_5000_movies.csv',
        'tmdb_5000_credits.csv': 'https://github.com/bprzybysz/neo4j/files/11600276/tmdb_5000_credits.csv'
    }
    print("Downloading TMDB dataset...")
    for filename, url in urls.items():
        dest_path = data_dir / filename
        if dest_path.exists():
            print(f"{filename} already exists in data/raw/")
            continue
        temp_path = temp_dir / filename
        print(f"Downloading {filename}...")
        try:
            # Try the primary URL
            urllib.request.urlretrieve(url, temp_path)
        except Exception as e:
            print(f"Error downloading from primary URL: {e}")
            print(f"Trying backup URL...")
            try:
                # Try the backup URL
                backup_url = backup_urls.get(filename)
                if backup_url:
                    urllib.request.urlretrieve(backup_url, temp_path)
                else:
                    print(f"No backup URL for {filename}")
                    continue
            except Exception as e:
                print(f"Error downloading from backup URL: {e}")
                print(f"Creating empty file for {filename} to allow testing")
                # Create an empty file for testing
                with open(temp_path, 'w') as f:
                    if filename == 'tmdb_5000_movies.csv':
                        f.write("id,title,release_date,budget,revenue,popularity,vote_average,vote_count,overview,genres,keywords,production_companies\n")
                        f.write("1,Test Movie,2023-01-01,1000000,2000000,7.5,8.0,100,Test overview,[{\"id\": 1, \"name\": \"Action\"}],[{\"id\": 1, \"name\": \"hero\"}],[{\"id\": 1, \"name\": \"Test Studio\", \"origin_country\": \"US\"}]\n")
                    elif filename == 'tmdb_5000_credits.csv':
                        f.write("id,cast,crew\n")
                        f.write("1,[{\"id\": 1, \"name\": \"Test Actor\", \"gender\": 1, \"character\": \"Main Character\"}],[{\"id\": 2, \"name\": \"Test Director\", \"gender\": 2, \"job\": \"Director\", \"department\": \"Directing\"}]\n")
        # Move file to final destination
        print(f"Moving {filename} to data/raw/")
        shutil.move(str(temp_path), str(dest_path))
    # Clean up temp directory
    shutil.rmtree(temp_dir)
    print("Dataset downloaded and moved successfully!")
if __name__ == "__main__":
    download_tmdb_dataset()
</file>

<file path="scripts/etl_process.py">
#!/usr/bin/env python3
"""
ETL script to transform TMDB movie dataset into Neo4j importable format.
"""
import os
import sys
import argparse
# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from movie_graph.etl.process import run_etl
def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Transform movie data for Neo4j import')
    parser.add_argument('--input', type=str, default='data/raw',
                        help='Directory containing input files')
    parser.add_argument('--output', type=str, default='data/processed',
                        help='Directory to save output files')
    parser.add_argument('--movies', type=str, default='tmdb_5000_movies.csv',
                        help='Filename for movies data')
    parser.add_argument('--credits', type=str, default='tmdb_5000_credits.csv',
                        help='Filename for credits data')
    return parser.parse_args()
def main():
    """Main ETL process."""
    args = parse_args()
    # Run ETL process
    run_etl(
        input_dir=args.input,
        output_dir=args.output,
        movies_file=args.movies,
        credits_file=args.credits
    )
if __name__ == "__main__":
    main()
</file>

<file path="scripts/fix_notebooks.py">
#!/usr/bin/env python
"""
Jupyter Notebook Fixer Script
This script fixes common issues in Jupyter notebooks (.ipynb files) to ensure 
they meet the requirements for both Cursor and Spark environments.
"""
import json
import os
import sys
import uuid
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import nbformat
from nbformat.validator import validate
def load_notebook(notebook_path: str) -> Tuple[Dict[str, Any], bool]:
    """
    Loads a notebook from the given path.
    Args:
        notebook_path: Path to the notebook file
    Returns:
        Tuple of (notebook_content, success)
    """
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook_content = json.load(f)
        return notebook_content, True
    except Exception as e:
        print(f"Error loading notebook {notebook_path}: {str(e)}")
        return {}, False
def save_notebook(notebook_path: str, notebook_content: Dict[str, Any]) -> bool:
    """
    Saves a notebook to the given path.
    Args:
        notebook_path: Path to save the notebook
        notebook_content: Notebook content to save
    Returns:
        Success status
    """
    try:
        # Ensure the notebook is valid before saving
        nb = nbformat.reads(json.dumps(notebook_content), as_version=4)
        validate(nb)
        with open(notebook_path, 'w', encoding='utf-8') as f:
            json.dump(notebook_content, f, indent=1)
        return True
    except Exception as e:
        print(f"Error saving notebook {notebook_path}: {str(e)}")
        return False
def fix_notebook_structure(notebook_content: Dict[str, Any]) -> Dict[str, Any]:
    """
    Fixes structural issues in the notebook.
    Args:
        notebook_content: The notebook content
    Returns:
        Fixed notebook content
    """
    # Fix nbformat version
    notebook_content['nbformat'] = 4
    notebook_content['nbformat_minor'] = 5
    # Fix or add kernelspec
    if 'metadata' not in notebook_content:
        notebook_content['metadata'] = {}
    if 'kernelspec' not in notebook_content['metadata']:
        notebook_content['metadata']['kernelspec'] = {
            'display_name': 'Python 3',
            'language': 'python',
            'name': 'python3'
        }
    else:
        # Ensure kernel is Python-based
        kernelspec = notebook_content['metadata']['kernelspec']
        if not kernelspec.get('name') or 'python' not in kernelspec.get('name', '').lower():
            kernelspec['name'] = 'python3'
            kernelspec['display_name'] = 'Python 3'
            kernelspec['language'] = 'python'
    # Ensure cells array exists
    if 'cells' not in notebook_content:
        notebook_content['cells'] = []
    # Fix cell structures
    for i, cell in enumerate(notebook_content['cells']):
        # Fix missing cell type
        if 'cell_type' not in cell:
            # Default to code cell if source contains Python-like code indicators
            source = ''.join(cell.get('source', []))
            if any(kw in source for kw in ['import ', 'def ', 'class ', '= ', 'print(']):
                cell['cell_type'] = 'code'
            else:
                cell['cell_type'] = 'markdown'
        # Add metadata if missing
        if 'metadata' not in cell:
            cell['metadata'] = {}
        # Add cell ID if missing
        if 'id' not in cell['metadata']:
            cell['metadata']['id'] = str(uuid.uuid4())
        # Ensure source is present
        if 'source' not in cell:
            cell['source'] = []
        # Fix pyspark magic commands
        if cell['cell_type'] == 'code':
            source = ''.join(cell['source']) if isinstance(cell['source'], list) else cell['source']
            if 'spark.sql' in source and not source.strip().startswith('%%pyspark'):
                if isinstance(cell['source'], list):
                    cell['source'] = ['%%pyspark\n'] + cell['source']
                else:
                    cell['source'] = f"%%pyspark\n{cell['source']}"
    return notebook_content
def fix_spark_compatibility(notebook_content: Dict[str, Any]) -> Dict[str, Any]:
    """
    Fixes Spark compatibility issues in the notebook.
    Args:
        notebook_content: The notebook content
    Returns:
        Fixed notebook content
    """
    cells = notebook_content.get('cells', [])
    # Check if notebook uses Spark
    uses_spark = any('spark.' in ''.join(cell.get('source', []))
                    if cell.get('cell_type') == 'code' else False 
                    for cell in cells)
    if not uses_spark:
        return notebook_content
    # Check if SparkSession is initialized
    spark_initialized = any('SparkSession' in ''.join(cell.get('source', []))
                         and '.builder' in ''.join(cell.get('source', []))
                         if cell.get('cell_type') == 'code' else False
                         for cell in cells)
    if not spark_initialized:
        # Add SparkSession initialization cell at the beginning
        spark_init_cell = {
            'cell_type': 'code',
            'metadata': {'id': str(uuid.uuid4())},
            'source': [
                '# Initialize Spark Session\n',
                'from pyspark.sql import SparkSession\n',
                '\n',
                'spark = SparkSession.builder \\\n',
                '    .appName("Notebook") \\\n',
                '    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \\\n',
                '    .getOrCreate()\n'
            ],
            'execution_count': None,
            'outputs': []
        }
        # Find the best position to insert - after imports but before other code
        inserted = False
        for i, cell in enumerate(cells):
            if cell.get('cell_type') != 'code':
                continue
            source = ''.join(cell.get('source', []))
            if 'import ' in source and not any(x in source for x in [
                'spark.', 'SparkContext', 'SparkSession'
            ]):
                notebook_content['cells'].insert(i + 1, spark_init_cell)
                inserted = True
                break
        if not inserted:
            # Insert after the first markdown cell or at the beginning
            for i, cell in enumerate(cells):
                if cell.get('cell_type') == 'markdown':
                    notebook_content['cells'].insert(i + 1, spark_init_cell)
                    inserted = True
                    break
            if not inserted:
                notebook_content['cells'].insert(0, spark_init_cell)
    # Fix DataFrame caching
    for i, cell in enumerate(notebook_content['cells']):
        if cell.get('cell_type') != 'code':
            continue
        source = ''.join(cell.get('source', []))
        if 'spark.read' in source and '.cache()' not in source and '.persist(' not in source:
            # Add caching to dataframe assignment lines
            lines = source.split('\n')
            modified_lines = []
            for line in lines:
                if '=' in line and 'spark.read' in line:
                    var_name = line.split('=')[0].strip()
                    modified_lines.append(line)
                    modified_lines.append(f"{var_name} = {var_name}.cache()  # Improved performance with caching")
                else:
                    modified_lines.append(line)
            if isinstance(cell['source'], list):
                cell['source'] = [line + '\n' for line in modified_lines[:-1]] + [modified_lines[-1]]
            else:
                cell['source'] = '\n'.join(modified_lines)
    return notebook_content
def fix_cursor_integration(notebook_content: Dict[str, Any]) -> Dict[str, Any]:
    """
    Fixes Cursor integration issues in the notebook.
    Args:
        notebook_content: The notebook content
    Returns:
        Fixed notebook content
    """
    cells = notebook_content.get('cells', [])
    # Check if notebook has a title
    has_header = any(
        cell.get('cell_type') == 'markdown' and 
        ''.join(cell.get('source', [])).startswith('# ')
        for cell in cells
    )
    if not has_header:
        # Extract a potential title from the filename or use a default
        notebook_path = notebook_content.get('_file_path', 'Notebook')
        if notebook_path:
            title = os.path.splitext(os.path.basename(notebook_path))[0]
            title = title.replace('_', ' ').title()
        else:
            title = "Untitled Notebook"
        # Create title cell
        title_cell = {
            'cell_type': 'markdown',
            'metadata': {'id': str(uuid.uuid4())},
            'source': [f"# {title}\n", "\n", "This notebook was automatically formatted for Cursor and Spark compatibility.\n"]
        }
        # Insert at the beginning
        notebook_content['cells'].insert(0, title_cell)
    # Check for section headers
    has_sections = any(
        cell.get('cell_type') == 'markdown' and 
        ''.join(cell.get('source', [])).startswith('## ')
        for cell in cells
    )
    if not has_sections:
        # Add a default section before the first code cell
        for i, cell in enumerate(cells):
            if cell.get('cell_type') == 'code':
                section_cell = {
                    'cell_type': 'markdown',
                    'metadata': {'id': str(uuid.uuid4())},
                    'source': ["## Data Processing\n", "\n", "This section contains code for data processing.\n"]
                }
                notebook_content['cells'].insert(i, section_cell)
                break
    # Ensure all cells have IDs
    for cell in cells:
        if 'metadata' not in cell:
            cell['metadata'] = {}
        if 'id' not in cell['metadata']:
            cell['metadata']['id'] = str(uuid.uuid4())
    return notebook_content
def fix_notebook(notebook_path: str) -> bool:
    """
    Fixes issues in a notebook to make it Cursor and Spark compatible.
    Args:
        notebook_path: Path to the notebook file
    Returns:
        Success status
    """
    # Load the notebook
    notebook_content, success = load_notebook(notebook_path)
    if not success:
        return False
    # Store the original path for reference
    notebook_content['_file_path'] = notebook_path
    # Apply fixes
    notebook_content = fix_notebook_structure(notebook_content)
    notebook_content = fix_spark_compatibility(notebook_content)
    notebook_content = fix_cursor_integration(notebook_content)
    # Remove temporary field
    if '_file_path' in notebook_content:
        del notebook_content['_file_path']
    # Save the fixed notebook
    return save_notebook(notebook_path, notebook_content)
def main():
    """Main function to fix all notebooks."""
    notebooks_dir = Path('notebooks')
    if not notebooks_dir.exists():
        print(f"Error: Notebooks directory {notebooks_dir} not found")
        sys.exit(1)
    notebook_files = list(notebooks_dir.glob('**/*.ipynb'))
    if not notebook_files:
        print("No notebook files found to fix")
        sys.exit(0)
    print(f"Found {len(notebook_files)} notebook(s) to fix")
    all_fixed = True
    for notebook_path in notebook_files:
        # Skip checkpoints
        if '.ipynb_checkpoints' in str(notebook_path):
            print(f"Skipping checkpoint file: {notebook_path}")
            continue
        print(f"\nFixing {notebook_path}...")
        success = fix_notebook(str(notebook_path))
        if success:
            print(f" Successfully fixed {notebook_path}")
        else:
            all_fixed = False
            print(f" Failed to fix {notebook_path}")
    if all_fixed:
        print("\nAll notebooks have been fixed successfully!")
        sys.exit(0)
    else:
        print("\nSome notebooks could not be fixed. Please check the errors.")
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="scripts/requirements.txt">
nbformat>=5.7.0
jupyter>=1.0.0
pyspark>=3.3.0
pandas>=1.5.0
numpy>=1.22.0
matplotlib>=3.5.0
seaborn>=0.12.0
</file>

<file path="scripts/run_with_checks.py">
#!/usr/bin/env python3
"""
Wrapper script to run Python scripts with environment checks.
This ensures SSH key and virtual environment are properly set up before execution.
"""
import os
import sys
import subprocess
from pathlib import Path
from typing import List, Optional
def run_environment_checks() -> bool:
    """Run the environment check script."""
    check_script = Path(__file__).parent.parent / "check_environment.sh"
    try:
        subprocess.run(["bash", str(check_script)], check=True)
        return True
    except subprocess.CalledProcessError:
        print("Environment checks failed. Please fix the issues above.")
        return False
def run_script(script_path: str, args: List[str]) -> Optional[int]:
    """
    Run the specified Python script with given arguments.
    Args:
        script_path: Path to the Python script to run
        args: List of command line arguments for the script
    Returns:
        Return code from the script execution or None if checks failed
    """
    if not run_environment_checks():
        return None
    # Execute the target script
    try:
        result = subprocess.run([sys.executable, script_path] + args)
        return result.returncode
    except Exception as e:
        print(f"Error executing script: {e}")
        return 1
def main() -> int:
    """Main entry point."""
    if len(sys.argv) < 2:
        print("Usage: python run_with_checks.py <script_path> [args...]")
        return 1
    script_path = sys.argv[1]
    script_args = sys.argv[2:]
    if not os.path.exists(script_path):
        print(f"Error: Script not found: {script_path}")
        return 1
    result = run_script(script_path, script_args)
    return 0 if result is None else result
if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/test_etl_process.py">
"""
Test ETL process for transforming TMDB movie dataset into Neo4j importable format.
Uses a small subset of data for testing and includes additional validation.
"""
import os
import json
import pandas as pd
from typing import Dict, List, Any, Tuple
def ensure_dir(directory: str) -> None:
    """Create directory if it doesn't exist."""
    if not os.path.exists(directory):
        os.makedirs(directory)
def parse_json_fields(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
    """Parse JSON string fields into Python objects."""
    def safe_json_loads(x):
        if pd.isna(x):
            return []
        try:
            # Clean up the string representation
            x_str = str(x).strip()
            # Handle single quotes and other common issues
            x_str = x_str.replace("'", '"')
            x_str = x_str.replace("None", "null")
            x_str = x_str.replace("True", "true")
            x_str = x_str.replace("False", "false")
            return json.loads(x_str)
        except json.JSONDecodeError as e:
            print(f"Warning: Could not parse JSON: {x}")
            print(f"Error: {e}")
            return []
    for col in columns:
        df[col] = df[col].apply(safe_json_loads)
    return df
def transform_data(df: pd.DataFrame) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
    """Transform the data into Neo4j compatible format."""
    print("Transforming data...")
    # Extract nodes
    movies = []
    persons = {}  # Using dict to avoid duplicates
    genres = {}  # Using dict to avoid duplicates
    keywords = {}  # Using dict to avoid duplicates
    companies = {}  # Using dict to avoid duplicates
    # Extract relationships
    acted_in = []
    directed = []
    produced = []
    categorized_as = []
    tagged_with = []
    # Process each movie
    for _, row in df.iterrows():
        movie_id = row["id"]
        # Movie node
        movie = {
            "id": movie_id,
            "title": row["title"],
            "release_date": row["release_date"],
            "budget": row["budget"],
            "revenue": row["revenue"],
            "popularity": row["popularity"],
            "vote_average": row["vote_average"],
            "vote_count": row["vote_count"],
            "overview": row["overview"],
        }
        movies.append(movie)
        # Process genres
        for genre in row["genres"]:
            genre_id = genre["id"]
            genres[genre_id] = {"id": genre_id, "name": genre["name"]}
            categorized_as.append({"movie_id": movie_id, "genre_id": genre_id})
        # Process keywords
        for keyword in row["keywords"]:
            keyword_id = keyword["id"]
            keywords[keyword_id] = {"id": keyword_id, "name": keyword["name"]}
            tagged_with.append({"movie_id": movie_id, "keyword_id": keyword_id})
        # Process production companies
        for company in row["production_companies"]:
            company_id = company["id"]
            companies[company_id] = {
                "id": company_id,
                "name": company["name"],
                "origin_country": company.get("origin_country", ""),
            }
            produced.append({"movie_id": movie_id, "company_id": company_id})
        # Process cast (actors)
        for i, actor in enumerate(row["cast"][:10]):  # Limit to top 10 actors
            person_id = actor["id"]
            persons[person_id] = {
                "id": person_id,
                "name": actor["name"],
                "gender": actor.get("gender", 0),
                "profile_path": actor.get("profile_path", ""),
            }
            acted_in.append(
                {
                    "person_id": person_id,
                    "movie_id": movie_id,
                    "character": actor["character"],
                    "order": i,
                }
            )
        # Process crew (focus on directors)
        for crew_member in row["crew"]:
            if crew_member["job"] == "Director":
                person_id = crew_member["id"]
                persons[person_id] = {
                    "id": person_id,
                    "name": crew_member["name"],
                    "gender": crew_member.get("gender", 0),
                    "profile_path": crew_member.get("profile_path", ""),
                }
                directed.append(
                    {
                        "person_id": person_id,
                        "movie_id": movie_id,
                        "job": crew_member["job"],
                        "department": crew_member["department"],
                    }
                )
    # Convert dictionaries to lists
    genres_list = list(genres.values())
    keywords_list = list(keywords.values())
    companies_list = list(companies.values())
    persons_list = list(persons.values())
    transformed_data = {
        "nodes": {
            "movies": movies,
            "persons": persons_list,
            "genres": genres_list,
            "keywords": keywords_list,
            "companies": companies_list,
        },
        "relationships": {
            "acted_in": acted_in,
            "directed": directed,
            "produced": produced,
            "categorized_as": categorized_as,
            "tagged_with": tagged_with,
        },
    }
    return transformed_data
def load_data(
    data: Dict[str, Dict[str, List[Dict[str, Any]]]], output_dir: str
) -> None:
    """Save transformed data as CSV files for Neo4j import."""
    print("Loading data...")
    ensure_dir(output_dir)
    # Save nodes
    print("Saving nodes...")
    pd.DataFrame(data["nodes"]["movies"]).to_csv(
        os.path.join(output_dir, "movies.csv"), index=False
    )
    pd.DataFrame(data["nodes"]["persons"]).to_csv(
        os.path.join(output_dir, "persons.csv"), index=False
    )
    pd.DataFrame(data["nodes"]["genres"]).to_csv(
        os.path.join(output_dir, "genres.csv"), index=False
    )
    pd.DataFrame(data["nodes"]["keywords"]).to_csv(
        os.path.join(output_dir, "keywords.csv"), index=False
    )
    pd.DataFrame(data["nodes"]["companies"]).to_csv(
        os.path.join(output_dir, "companies.csv"), index=False
    )
    # Save relationships
    print("Saving relationships...")
    pd.DataFrame(data["relationships"]["acted_in"]).to_csv(
        os.path.join(output_dir, "acted_in.csv"), index=False
    )
    pd.DataFrame(data["relationships"]["directed"]).to_csv(
        os.path.join(output_dir, "directed.csv"), index=False
    )
    pd.DataFrame(data["relationships"]["produced"]).to_csv(
        os.path.join(output_dir, "produced.csv"), index=False
    )
    pd.DataFrame(data["relationships"]["categorized_as"]).to_csv(
        os.path.join(output_dir, "categorized_as.csv"), index=False
    )
    pd.DataFrame(data["relationships"]["tagged_with"]).to_csv(
        os.path.join(output_dir, "tagged_with.csv"), index=False
    )
def extract_test_data(
    input_dir: str, movies_file: str, credits_file: str, sample_size: int = 10
) -> Tuple[pd.DataFrame, List[str]]:
    """Extract a small sample of data from raw CSV files for testing."""
    print(f"Extracting test data (sample size: {sample_size})...")
    validation_msgs = []
    # Read sample of movies
    movies_df = pd.read_csv(os.path.join(input_dir, movies_file))
    movies_sample = movies_df.head(sample_size)
    validation_msgs.append(f"Sampled {len(movies_sample)} movies from {movies_file}")
    # Read corresponding credits
    credits_df = pd.read_csv(os.path.join(input_dir, credits_file))
    credits_sample = credits_df[credits_df["movie_id"].isin(movies_sample["id"])]
    validation_msgs.append(f"Found {len(credits_sample)} matching credit records")
    # Rename id column in credits to match movies
    if "movie_id" in credits_sample.columns:
        credits_sample.rename(columns={"movie_id": "id"}, inplace=True)
    # Merge dataframes
    df = pd.merge(movies_sample, credits_sample, on="id")
    validation_msgs.append(f"Final merged dataset has {len(df)} rows")
    # Validate JSON columns before parsing
    json_columns = ["genres", "keywords", "production_companies", "cast", "crew"]
    for col in json_columns:
        if col not in df.columns:
            validation_msgs.append(f"Warning: Column {col} not found in dataset")
            continue
        # Check for non-null values
        null_count = df[col].isnull().sum()
        if null_count > 0:
            validation_msgs.append(f"Warning: {null_count} null values found in {col}")
        # Sample and validate JSON structure
        sample_value = df[col].iloc[0]
        validation_msgs.append(f"Sample {col} value: {sample_value}")
    # Convert string representations of lists/dicts to actual Python objects
    df = parse_json_fields(df, json_columns)
    return df, validation_msgs
def run_test_etl(
    input_dir: str,
    output_dir: str,
    movies_file: str = "tmdb_5000_movies.csv",
    credits_file: str = "tmdb_5000_credits.csv",
    sample_size: int = 10,
) -> None:
    """Run the ETL process on a small sample for testing."""
    print("Starting test ETL process...")
    # Extract test data
    data_df, validation_msgs = extract_test_data(
        input_dir, movies_file, credits_file, sample_size
    )
    # Print validation messages
    print("\nValidation Messages:")
    for msg in validation_msgs:
        print(f"- {msg}")
    # Transform
    print("\nTransforming test data...")
    transformed_data = transform_data(data_df)
    # Validate transformed data
    print("\nValidating transformed data:")
    for node_type, nodes in transformed_data["nodes"].items():
        print(f"- {node_type}: {len(nodes)} nodes")
    for rel_type, rels in transformed_data["relationships"].items():
        print(f"- {rel_type}: {len(rels)} relationships")
    # Load
    test_output_dir = os.path.join(output_dir, "test")
    load_data(transformed_data, test_output_dir)
    print("\nTest ETL process completed successfully!")
    print(f"Test output files saved to: {test_output_dir}")
if __name__ == "__main__":
    # Run test ETL with 10 movies
    run_test_etl(input_dir="data/raw", output_dir="data/processed", sample_size=10)
</file>

<file path="scripts/validate_notebooks.py">
#!/usr/bin/env python
"""
Jupyter Notebook Validator Script
This script validates Jupyter notebooks (.ipynb files) to ensure they meet
the requirements for both Cursor and Spark environments.
"""
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import nbformat
from nbformat.validator import validate
def validate_notebook_structure(notebook_path: str) -> Tuple[bool, List[str]]:
    """
    Validates the basic structure of a Jupyter notebook.
    Args:
        notebook_path: Path to the notebook file
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook_content = json.load(f)
        # Validate with nbformat
        try:
            nb = nbformat.reads(json.dumps(notebook_content), as_version=4)
            validate(nb)
        except Exception as e:
            issues.append(f"nbformat validation failed: {str(e)}")
            return False, issues
        # Check nbformat version
        if notebook_content.get('nbformat') != 4:
            issues.append(f"nbformat should be 4, got {notebook_content.get('nbformat')}")
        if notebook_content.get('nbformat_minor', 0) < 5:
            issues.append(f"nbformat_minor should be at least 5, got {notebook_content.get('nbformat_minor')}")
        # Check if kernelspec is present and valid
        metadata = notebook_content.get('metadata', {})
        kernelspec = metadata.get('kernelspec', {})
        if not kernelspec:
            issues.append("Missing kernelspec in metadata")
        else:
            kernel_name = kernelspec.get('name')
            if not kernel_name:
                issues.append("Missing kernel name in kernelspec")
            elif "python" not in kernel_name.lower():
                issues.append(f"Kernel should be Python-based, got {kernel_name}")
        # Check cells structure
        cells = notebook_content.get('cells', [])
        if not cells:
            issues.append("Notebook has no cells")
        for i, cell in enumerate(cells):
            # Check cell type
            if 'cell_type' not in cell:
                issues.append(f"Cell {i} is missing cell_type")
            # Check cell metadata
            cell_metadata = cell.get('metadata', {})
            if not cell_metadata:
                issues.append(f"Cell {i} has no metadata")
            # Check if cell has source
            if 'source' not in cell:
                issues.append(f"Cell {i} is missing source content")
            # Check for code cells using pyspark
            if cell.get('cell_type') == 'code' and isinstance(cell.get('source'), list):
                source = ''.join(cell.get('source', []))
                if 'spark.sql' in source and not source.strip().startswith('%%pyspark'):
                    issues.append(f"Cell {i} contains Spark SQL but is missing %%pyspark magic")
        return len(issues) == 0, issues
    except Exception as e:
        issues.append(f"Failed to validate notebook: {str(e)}")
        return False, issues
def validate_spark_compatibility(notebook_path: str) -> Tuple[bool, List[str]]:
    """
    Validates if the notebook is compatible with Spark environment.
    Args:
        notebook_path: Path to the notebook file
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook_content = json.load(f)
        cells = notebook_content.get('cells', [])
        spark_session_initialized = False
        for i, cell in enumerate(cells):
            if cell.get('cell_type') != 'code':
                continue
            source = ''.join(cell.get('source', []))
            # Check for SparkSession initialization
            if 'SparkSession' in source and '.builder' in source:
                spark_session_initialized = True
            # Check for direct SparkContext usage without session
            if 'SparkContext' in source and not spark_session_initialized:
                issues.append(f"Cell {i} uses SparkContext directly without SparkSession")
            # Check for appropriate DataFrame caching
            if 'spark.read' in source and '.cache()' not in source and '.persist(' not in source:
                issues.append(f"Cell {i} loads data without caching strategy")
        if not spark_session_initialized and any('spark.' in ''.join(cell.get('source', [])) 
                                               for cell in cells 
                                               if cell.get('cell_type') == 'code'):
            issues.append("Notebook uses Spark but does not initialize SparkSession")
        return len(issues) == 0, issues
    except Exception as e:
        issues.append(f"Failed to validate Spark compatibility: {str(e)}")
        return False, issues
def validate_cursor_integration(notebook_path: str) -> Tuple[bool, List[str]]:
    """
    Validates if the notebook follows Cursor integration best practices.
    Args:
        notebook_path: Path to the notebook file
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []
    try:
        with open(notebook_path, 'r', encoding='utf-8') as f:
            notebook_content = json.load(f)
        cells = notebook_content.get('cells', [])
        has_header = False
        section_headers = 0
        for i, cell in enumerate(cells):
            cell_type = cell.get('cell_type')
            # Check for markdown cells with headers
            if cell_type == 'markdown':
                source = ''.join(cell.get('source', []))
                if source.startswith('# '):
                    has_header = True
                if source.startswith('## '):
                    section_headers += 1
            # Check for cell IDs in metadata
            cell_metadata = cell.get('metadata', {})
            if 'id' not in cell_metadata:
                issues.append(f"Cell {i} is missing an ID in metadata")
        # Ensure notebook has a title and sections
        if not has_header:
            issues.append("Notebook is missing a title (# Header)")
        if section_headers == 0:
            issues.append("Notebook has no section headers (## Section)")
        return len(issues) == 0, issues
    except Exception as e:
        issues.append(f"Failed to validate Cursor integration: {str(e)}")
        return False, issues
def main():
    """Main function to validate all notebooks."""
    notebooks_dir = Path('notebooks')
    if not notebooks_dir.exists():
        print(f"Error: Notebooks directory {notebooks_dir} not found")
        sys.exit(1)
    notebook_files = list(notebooks_dir.glob('**/*.ipynb'))
    if not notebook_files:
        print("No notebook files found to validate")
        sys.exit(0)
    print(f"Found {len(notebook_files)} notebook(s) to validate")
    all_valid = True
    for notebook_path in notebook_files:
        print(f"\nValidating {notebook_path}...")
        # Skip checkpoints
        if '.ipynb_checkpoints' in str(notebook_path):
            print(f"Skipping checkpoint file: {notebook_path}")
            continue
        # Validate structure
        structure_valid, structure_issues = validate_notebook_structure(str(notebook_path))
        if not structure_valid:
            all_valid = False
            print(" Structure validation failed:")
            for issue in structure_issues:
                print(f"  - {issue}")
        else:
            print(" Structure validation passed")
        # Validate Spark compatibility
        spark_valid, spark_issues = validate_spark_compatibility(str(notebook_path))
        if not spark_valid:
            all_valid = False
            print(" Spark compatibility validation failed:")
            for issue in spark_issues:
                print(f"  - {issue}")
        else:
            print(" Spark compatibility validation passed")
        # Validate Cursor integration
        cursor_valid, cursor_issues = validate_cursor_integration(str(notebook_path))
        if not cursor_valid:
            all_valid = False
            print(" Cursor integration validation failed:")
            for issue in cursor_issues:
                print(f"  - {issue}")
        else:
            print(" Cursor integration validation passed")
    if all_valid:
        print("\nAll notebooks are valid!")
        sys.exit(0)
    else:
        print("\nSome notebooks have validation issues. Please fix them or run the fix_notebooks.py script.")
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="tests/test_db.py">
"""
Tests for the database connection module.
"""
import pytest
from unittest.mock import patch, MagicMock
import pandas as pd
from movie_graph.db.connection import Neo4jConnection, get_connection
class TestNeo4jConnection:
    """Tests for the Neo4jConnection class."""
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_init(self, mock_graph_db):
        """Test initialization of Neo4jConnection."""
        # Arrange
        mock_driver = MagicMock()
        mock_graph_db.driver.return_value = mock_driver
        # Act
        conn = Neo4jConnection("bolt://localhost:7687", ("neo4j", "password"))
        # Assert
        mock_graph_db.driver.assert_called_once_with("bolt://localhost:7687", auth=("neo4j", "password"))
        assert conn.driver == mock_driver
        assert conn.database is None
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_init_with_database(self, mock_graph_db):
        """Test initialization of Neo4jConnection with database."""
        # Arrange
        mock_driver = MagicMock()
        mock_graph_db.driver.return_value = mock_driver
        # Act
        conn = Neo4jConnection("bolt://localhost:7687", ("neo4j", "password"), "movies")
        # Assert
        mock_graph_db.driver.assert_called_once_with("bolt://localhost:7687", auth=("neo4j", "password"))
        assert conn.driver == mock_driver
        assert conn.database == "movies"
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_close(self, mock_graph_db):
        """Test close method."""
        # Arrange
        mock_driver = MagicMock()
        mock_graph_db.driver.return_value = mock_driver
        conn = Neo4jConnection("bolt://localhost:7687", ("neo4j", "password"))
        # Act
        conn.close()
        # Assert
        mock_driver.close.assert_called_once()
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_query(self, mock_graph_db):
        """Test query method."""
        # Arrange
        mock_driver = MagicMock()
        mock_session = MagicMock()
        mock_result = MagicMock()
        mock_result.keys.return_value = ["name", "count"]
        mock_result.__iter__.return_value = [
            MagicMock(values=lambda: ["John", 10]),
            MagicMock(values=lambda: ["Jane", 20])
        ]
        mock_session.run.return_value = mock_result
        mock_session.__enter__.return_value = mock_session
        mock_driver.session.return_value = mock_session
        mock_graph_db.driver.return_value = mock_driver
        conn = Neo4jConnection("bolt://localhost:7687", ("neo4j", "password"))
        # Act
        df = conn.query("MATCH (n) RETURN n.name as name, count(*) as count")
        # Assert
        mock_session.run.assert_called_once_with("MATCH (n) RETURN n.name as name, count(*) as count", {})
        assert isinstance(df, pd.DataFrame)
        assert list(df.columns) == ["name", "count"]
        assert df.shape == (2, 2)
        assert df.iloc[0, 0] == "John"
        assert df.iloc[0, 1] == 10
        assert df.iloc[1, 0] == "Jane"
        assert df.iloc[1, 1] == 20
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_query_with_parameters(self, mock_graph_db):
        """Test query method with parameters."""
        # Arrange
        mock_driver = MagicMock()
        mock_session = MagicMock()
        mock_result = MagicMock()
        mock_result.keys.return_value = ["name"]
        mock_result.__iter__.return_value = [
            MagicMock(values=lambda: ["John"])
        ]
        mock_session.run.return_value = mock_result
        mock_session.__enter__.return_value = mock_session
        mock_driver.session.return_value = mock_session
        mock_graph_db.driver.return_value = mock_driver
        conn = Neo4jConnection("bolt://localhost:7687", ("neo4j", "password"))
        # Act
        df = conn.query("MATCH (n) WHERE n.name = $name RETURN n.name as name", {"name": "John"})
        # Assert
        mock_session.run.assert_called_once_with(
            "MATCH (n) WHERE n.name = $name RETURN n.name as name", 
            {"name": "John"}
        )
        assert isinstance(df, pd.DataFrame)
        assert list(df.columns) == ["name"]
        assert df.shape == (1, 1)
        assert df.iloc[0, 0] == "John"
    @patch('movie_graph.db.connection.GraphDatabase')
    def test_context_manager(self, mock_graph_db):
        """Test Neo4jConnection as context manager."""
        # Arrange
        mock_driver = MagicMock()
        mock_graph_db.driver.return_value = mock_driver
        # Act
        with Neo4jConnection("bolt://localhost:7687", ("neo4j", "password")) as conn:
            pass
        # Assert
        mock_driver.close.assert_called_once()
    @patch('movie_graph.db.connection.Neo4jConnection')
    def test_get_connection(self, mock_connection_class):
        """Test get_connection function."""
        # Arrange
        mock_connection = MagicMock()
        mock_connection_class.return_value = mock_connection
        # Act
        conn = get_connection("bolt://localhost:7687", ("neo4j", "password"))
        # Assert
        mock_connection_class.assert_called_once_with("bolt://localhost:7687", ("neo4j", "password"), None)
        assert conn == mock_connection
</file>

<file path="tests/test_etl.py">
"""
Tests for the ETL module.
"""
import os
import tempfile
import pandas as pd
import pytest
from movie_graph.etl.process import (
    ensure_dir,
    parse_json_fields,
    extract_data,
    transform_data,
    load_data
)
def test_ensure_dir():
    """Test that ensure_dir creates directories correctly."""
    with tempfile.TemporaryDirectory() as tmpdir:
        test_dir = os.path.join(tmpdir, 'test_dir')
        # Directory shouldn't exist yet
        assert not os.path.exists(test_dir)
        # Create directory
        ensure_dir(test_dir)
        # Directory should exist now
        assert os.path.exists(test_dir)
        assert os.path.isdir(test_dir)
def test_parse_json_fields():
    """Test that parse_json_fields correctly parses JSON strings."""
    # Create test dataframe
    df = pd.DataFrame({
        'id': [1, 2],
        'json_field': ['[{"id": 1, "name": "test1"}]', '[{"id": 2, "name": "test2"}]']
    })
    # Parse JSON fields
    result_df = parse_json_fields(df, ['json_field'])
    # Check results
    assert isinstance(result_df['json_field'][0], list)
    assert result_df['json_field'][0][0]['id'] == 1
    assert result_df['json_field'][0][0]['name'] == 'test1'
    assert result_df['json_field'][1][0]['id'] == 2
    assert result_df['json_field'][1][0]['name'] == 'test2'
@pytest.mark.skip(reason="Requires actual data files")
def test_extract_data():
    """Test that extract_data correctly reads and merges data."""
    # This would require actual data files
    pass
def test_transform_data():
    """Test that transform_data correctly transforms the data."""
    # Create a minimal test dataframe
    df = pd.DataFrame({
        'id': [1],
        'title': ['Test Movie'],
        'release_date': ['2023-01-01'],
        'budget': [1000000],
        'revenue': [2000000],
        'popularity': [7.5],
        'vote_average': [8.0],
        'vote_count': [100],
        'overview': ['Test overview'],
        'genres': [[{'id': 1, 'name': 'Action'}]],
        'keywords': [[{'id': 1, 'name': 'hero'}]],
        'production_companies': [[{'id': 1, 'name': 'Test Studio', 'origin_country': 'US'}]],
        'cast': [[{'id': 1, 'name': 'Test Actor', 'gender': 1, 'character': 'Main Character'}]],
        'crew': [[{'id': 2, 'name': 'Test Director', 'gender': 2, 'job': 'Director', 'department': 'Directing'}]]
    })
    # Transform data
    result = transform_data(df)
    # Check structure
    assert 'nodes' in result
    assert 'relationships' in result
    # Check nodes
    assert 'movies' in result['nodes']
    assert 'persons' in result['nodes']
    assert 'genres' in result['nodes']
    assert 'keywords' in result['nodes']
    assert 'companies' in result['nodes']
    # Check relationships
    assert 'acted_in' in result['relationships']
    assert 'directed' in result['relationships']
    assert 'produced' in result['relationships']
    assert 'categorized_as' in result['relationships']
    assert 'tagged_with' in result['relationships']
    # Check content
    assert len(result['nodes']['movies']) == 1
    assert result['nodes']['movies'][0]['title'] == 'Test Movie'
    assert len(result['nodes']['persons']) == 2
    assert result['nodes']['persons'][0]['name'] in ['Test Actor', 'Test Director']
    assert result['nodes']['persons'][1]['name'] in ['Test Actor', 'Test Director']
    assert len(result['nodes']['genres']) == 1
    assert result['nodes']['genres'][0]['name'] == 'Action'
    assert len(result['relationships']['acted_in']) == 1
    assert len(result['relationships']['directed']) == 1
@pytest.mark.skip(reason="Creates output files")
def test_load_data():
    """Test that load_data correctly saves the data."""
    # This would create output files, so we'll skip it for now
    pass
</file>

<file path="workflow/codemap.md">
# Neo4j Movie Analysis Codemap

## Core Components

### 1. ETL Process
- **Entry Point**: `scripts/etl_process.py`
- **Functions**:
  - `extract_data()`: Reads data from CSV files
  - `transform_data()`: Transforms movie data into Neo4j format
  - `load_data()`: Saves processed data as CSV files for Neo4j import
  - `main()`: Orchestrates the ETL flow

### 2. Database Connection
- **Class**: `Neo4jConnection` (from notebooks/movie_analysis.ipynb)
- **Methods**:
  - `__init__(uri, auth)`: Initializes connection
  - `close()`: Closes connection
  - `query(query, parameters)`: Executes Cypher query

### 3. Notebook Conversion
- **Script**: `convert_notebook.py`
- **Purpose**: Converts text file to Jupyter notebook format

## Data Flow

```
Raw CSV Files > ETL Process > Processed CSV Files > Neo4j Import
                     
                     v
                  Analysis Notebooks > Visualizations/Reports
```

## Dependencies

- **neo4j**: Neo4j database driver
- **pandas**: Data processing and manipulation
- **matplotlib/seaborn**: Visualization
- **jupyter**: Notebook environment

## Key Files

1. **ETL Script** (`scripts/etl_process.py`):
   - Contains the complete ETL pipeline for processing movie data
   - Functions: extract_data, transform_data, load_data

2. **Movie Analysis Notebook** (`notebooks/movie_analysis.ipynb`):
   - Performs analysis on the Neo4j database
   - Contains Neo4jConnection class
   - Visualizes movie data relationships

3. **Notebook Converter** (`convert_notebook.py`):
   - Utility to convert text files to Jupyter notebooks
   - Used for creating analysis notebooks from templates

## Code Flow

1. **Data Ingestion**:
   - Raw CSV files are read from `data/raw`
   - `extract_data()` reads and merges movies and credits data

2. **Data Transformation**:
   - `transform_data()` converts to node/relationship structure
   - Entities: movies, persons, genres, keywords, companies
   - Relationships: acted_in, directed, produced, categorized_as, tagged_with

3. **Data Loading**:
   - Processed data saved to `data/processed`
   - Individual CSV files for each node/relationship type

4. **Data Analysis**:
   - Connect to Neo4j using `Neo4jConnection`
   - Execute Cypher queries to analyze the graph
   - Create visualizations of relationships and patterns

## Future Development

1. **Package Structure**:
   - Move ETL code to dedicated module
   - Create reusable Neo4j connection class
   - Add utility functions for common operations

2. **Testing**:
   - Add unit tests for ETL functions
   - Add integration tests for Neo4j connectivity

3. **Documentation**:
   - Create comprehensive API documentation
   - Add usage examples for common workflows
</file>

<file path="workflow/plan.md">
# Neo4j Movie Analysis Project Plan

## Project Overview
A Python-based project for analyzing movie data using Neo4j graph database. This project includes ETL scripts, Jupyter notebooks for analysis, and visualization tools.

## Project Structure
```
neo4j-movie-analysis/
 .venv/                  # Virtual environment
 .vscode/                # VS Code settings
 .cursorrules            # Cursor IDE rules for Python
 .gitignore              # Git ignore file
 README.md               # Project documentation
 requirements.txt        # Project dependencies
 data/                   # Data directory
    raw/                # Raw data files
    processed/          # Processed data for Neo4j import
 notebooks/              # Jupyter notebooks
    movie_analysis.ipynb    # Main analysis notebook
    cypher/             # Cypher query examples
 movie_graph/            # Main package directory
    __init__.py         # Package initialization
    etl/                # ETL code
       __init__.py
       process.py      # ETL processing code
    db/                 # Database connectivity
       __init__.py
       connection.py   # Neo4j connection class
    utils/              # Utility functions
        __init__.py
        helpers.py      # Helper functions
 scripts/                # Utility scripts
    convert_notebook.py # Notebook conversion script
    etl_process.py      # ETL process script
 tests/                  # Test suite
     __init__.py
     test_etl.py         # ETL tests
     test_db.py          # Database connection tests
```

## Tasks

### 1. Environment Setup
- [x] Set up virtual environment (.venv)
- [x] Create requirements.txt with dependencies
- [x] Update .cursorrules for Python development

### 2. Project Organization
- [ ] Create main package structure
- [ ] Move existing scripts to appropriate locations
- [ ] Fix imports and references

### 3. Code Quality
- [ ] Add type hints to functions
- [ ] Add docstrings to new code
- [ ] Create basic tests

### 4. Documentation
- [ ] Create comprehensive README.md
- [ ] Document data model
- [ ] Add usage examples

### 5. Neo4j Integration
- [ ] Fix connection class in notebooks
- [ ] Create standardized db connection module

## Workflow

1. **Setup Phase**
   - Recreate virtual environment
   - Install dependencies
   - Configure VS Code settings

2. **Refactoring Phase**
   - Create package structure
   - Move code to appropriate modules
   - Fix imports and references

3. **Testing Phase**
   - Add basic tests
   - Validate Neo4j connectivity

4. **Documentation Phase**
   - Add README and examples
   - Document project structure

## Progress Tracking

- **Setup**: 100%
- **Refactoring**: 0%
- **Testing**: 0%
- **Documentation**: 10%

## Next Steps
1. Create the package structure
2. Move etl_process.py to movie_graph/etl/process.py
3. Create Neo4j connection class in movie_graph/db/connection.py
4. Add basic tests for ETL and db connection
5. Complete README.md with usage examples
</file>

<file path="workflow/progression.md">
# Project Reorganization Progress

## Completed Tasks
- [x] Created workflow directory with plan and codemap
- [x] Updated .cursorrules to use Python settings instead of Java
- [x] Created requirements.txt with all necessary dependencies
- [x] Analyzed existing code structure
- [x] Created basic directory structure (movie_graph package)
- [x] Created Neo4jConnection class module
- [x] Moved ETL code to proper module
- [x] Fixed imports and references
- [x] Added proper type hints to functions
- [x] Added docstrings to new code
- [x] Created basic tests
- [x] Created README.md
- [x] Removed redundant files from root directory
- [x] Created updated notebook using the package

## In Progress
- [ ] Setting up virtual environment

## Pending Tasks
- [ ] Add comprehensive example code to README

## Milestones

### Milestone 1: Initial Setup (COMPLETED)
- Requirements and dependencies documented
- Project plan created
- Development rules configured

### Milestone 2: Code Organization (COMPLETED)
- Package structure created
- Code properly organized in modules
- Imports and references fixed
- Redundant code removed

### Milestone 3: Quality Improvements (COMPLETED)
- Type hints added
- Docstrings added
- Tests created

### Milestone 4: Documentation (PARTIALLY COMPLETED)
- README.md created
- Usage examples added
- API documentation still in progress

## Timeline
- Initial Setup: April 8, 2023 (COMPLETED)
- Code Organization: April 8, 2023 (COMPLETED)
- Quality Improvements: April 8, 2023 (COMPLETED)
- Documentation: April 9, 2023 (IN PROGRESS)
- Cleanup: April 9, 2023 (COMPLETED)

## Next Actions
1. Create a new virtual environment with the right dependencies
2. Add more examples to the README
3. Run tests to ensure everything works
</file>

<file path="workflow/summary.md">
# Project Reorganization Summary

## What Has Been Done

### 1. Structure and Organization
- Created a proper Python package structure with `movie_graph` as the main package
- Organized code into logical modules (db, etl, utils)
- Created standardized interfaces between components
- Set up directory structure for tests, data, and notebooks
- Removed redundant files and code from the project

### 2. Code Quality Improvements
- Added type hints to all functions for better static analysis
- Added comprehensive docstrings to all code
- Extracted reusable components into dedicated modules
- Created unit tests for key functionality

### 3. Project Setup
- Created `requirements.txt` with all dependencies
- Added setup scripts for easy environment creation
- Added test runner script with coverage reporting
- Updated .cursorrules to use Python-specific settings

### 4. Documentation
- Created comprehensive README.md with usage examples
- Added detailed comments to all code
- Created workflow documentation and planning
- Maintained the existing data model documentation

### 5. Workflow Improvements
- Added proper CLI arguments to scripts
- Improved error handling
- Made scripts more configurable and reusable
- Set up development workflow documentation
- Created updated notebook that uses the package API

## Project Structure

The project has been organized into the following structure:

```
neo4j-movie-analysis/
 data/                   # Data directory
    raw/                # Raw data files
    processed/          # Processed data for Neo4j import
 movie_graph/            # Main package directory
    __init__.py         # Package initialization
    etl/                # ETL code
       __init__.py
       process.py      # ETL processing code
    db/                 # Database connectivity
       __init__.py
       connection.py   # Neo4j connection class
    utils/              # Utility functions
        __init__.py
        helpers.py      # Helper functions
 notebooks/              # Jupyter notebooks
    movie_analysis.ipynb        # Original analysis notebook
    movie_analysis_updated.ipynb  # Updated notebook using the package
    cypher/             # Cypher query examples
 scripts/                # Utility scripts
    convert_notebook.py # Notebook conversion script
    etl_process.py      # ETL process script
 tests/                  # Test suite
    __init__.py
    test_etl.py         # ETL tests
    test_db.py          # Database connection tests
 workflow/               # Project workflow documentation
    plan.md             # Project plan
    codemap.md          # Code structure map
    progression.md      # Progress tracking
    summary.md          # This summary
 .venv/                  # Virtual environment
 .cursorrules            # Cursor IDE Python rules
 README.md               # Project documentation
 requirements.txt        # Project dependencies
 setup_venv.sh           # Script to set up virtual environment
 run_tests.sh            # Script to run tests
```

## How to Use the New Structure

### Setting Up

1. Run the setup script to create a virtual environment:
   ```
   ./setup_venv.sh
   ```

2. Activate the virtual environment:
   ```
   source .venv/bin/activate
   ```

### Running ETL Process

Use the ETL script with the new package structure:
```
python scripts/etl_process.py --input data/raw --output data/processed
```

### Running Tests

Use the test runner script:
```
./run_tests.sh
```

### Using the API in Your Code

```python
from movie_graph import connect_to_neo4j, run_etl

# Connect to Neo4j
conn = connect_to_neo4j("bolt://localhost:7687", ("neo4j", "password"))

# Run ETL process
run_etl("data/raw", "data/processed")

# Query the database
df = conn.query("MATCH (m:Movie) RETURN m.title, m.release_date LIMIT 10")
print(df)
```

## Next Steps

1. Update notebooks to use the new package structure (COMPLETED)
2. Create more comprehensive documentation of the API
3. Expand test coverage
4. Add more Cypher query examples
</file>

<file path=".cursorignore">
# Cursor-specific ignores
.cursor/
.cursor-cache/
.cursor-test-cache/

# Large data directories
data/
neo4j-data/
import/
logs/

# Generated files
**/__pycache__/
**/*.pyc
**/*.pyo
**/*.pyd
**/.ipynb_checkpoints/
**/.pytest_cache/

# Build and dist
build/
dist/
*.egg-info/

# Environment
.env
.venv/
env/
venv/

# IDE specific
.idea/
.vscode/
*.swp
*.swo

# Temporary and backup files
tmp/
.stash/
*.bak
*.tmp
*.log

# Large binary files and datasets
*.csv
*.parquet
*.arrow
*.feather
!example_data/*.csv

# Documentation
_build/
_static/
_templates/

# Keep these files for indexing
!README.md
!data_model.md
!cypher/*.cypher
!scripts/*.py
!notebooks/*.ipynb
!example_data/README.md

# Project configuration
!.cursorrules
!pyproject.toml
!requirements.txt
!setup.py
</file>

<file path=".cursorrules">
{
    "preferences": {
        "development": {
            "requireTests": true,
            "useTypeHints": true,
            "enforceDocumentation": {
                "newCode": true,
                "existingCode": false
            },
            "skipRefactoring": true,
            "allowLegacyPatterns": true,
            "pythonVersion": "3.11",
            "codingStyle": {
                "useClasses": true,
                "formatting": {
                    "indentSize": 4,
                    "lineLength": 88,
                    "useSpaces": true
                }
            }
        },
        "contextManagement": {
            "indexEntireCodebase": false,
            "useFileReferences": true
        },
        "codeQuality": {
            "criticalViolationsOnly": true,
            "newCodeStandards": "strict",
            "existingCodeStandards": "relaxed",
            "linters": {
                "python": {
                    "plugin": "ms-python.python",
                    "enabled": true,
                    "config": {
                        "python.linting.enabled": true,
                        "python.linting.pylintEnabled": true,
                        "python.linting.flake8Enabled": true,
                        "python.linting.mypyEnabled": true,
                        "python.formatting.provider": "black"
                    }
                }
            }
        },
        "models": {
            "simple": "gpt-4o-mini",
            "default": "claude-3.5-sonnet",
            "complex": "deepseek-r1",
            "backup": "gpt-4o",
            "autoModel": {
                "enabled": true,
                "allowedModels": [
                    "claude-3.5-sonnet",
                    "claude-3.7-sonnet",
                    "claude-3.7-sonnet-max",
                    "deepseek-r1",
                    "deepseek-v3",
                    "gemini-2.0-flash",
                    "gemini-2.0-flash-thinking-exp",
                    "gpt-4o",
                    "gpt-4o-mini",
                    "o1",
                    "o3-mini"
                ],
                "disallowedModels": [
                    "cursor-small",
                    "cursor-fast"
                ]
            }
        },
        "performance": {
            "useAgentMode": true,
            "maxToolCalls": 25,
            "contextPulling": true
        },
        "autoRun": true,
        "notifyOnSuccess": false,
        "notifyOnFailure": true,
        "venvPath": ".venv",
        "testing": {
            "framework": "pytest",
            "requireUnitTests": true,
            "requireIntegrationTests": true,
            "coverage": {
                "tool": "coverage",
                "minThreshold": 80
            }
        },
        "jupyter": {
            "notebookFormat": {
                "nbformat": 4,
                "nbformat_minor": 5,
                "requireKernelspec": true,
                "defaultKernel": "python3",
                "validateStructure": true
            },
            "sparkIntegration": {
                "useMagicCommands": true,
                "preferredMagic": "%%pyspark",
                "cacheDataFrames": true,
                "optimizationLevel": "auto"
            },
            "cursorIntegration": {
                "cellDelimiters": true,
                "requireHeaders": true,
                "requireCellIds": true
            }
        }
    },
    "rules": [
        {
            "name": "PythonLint",
            "trigger": "onCodeGeneration",
            "actions": [
                {
                    "type": "lint",
                    "command": ".venv/bin/python -m flake8",
                    "onSuccess": "RunTests",
                    "onFailure": "HandleLintErrors"
                }
            ]
        },
        {
            "name": "RunTests",
            "trigger": "onLintSuccess",
            "actions": [
                {
                    "type": "test",
                    "command": ".venv/bin/python -m pytest",
                    "onSuccess": "TypeCheck",
                    "onFailure": "HandleTestFailures"
                }
            ]
        },
        {
            "name": "TypeCheck",
            "trigger": "onTestSuccess",
            "actions": [
                {
                    "type": "analyze",
                    "command": ".venv/bin/python -m mypy .",
                    "onSuccess": "AutoReview",
                    "onFailure": "HandleTypeErrors"
                }
            ]
        },
        {
            "name": "HandleLintErrors",
            "trigger": "onLintFailure",
            "actions": [
                {
                    "type": "notify",
                    "message": "Linting failed. Attempting to fix..."
                },
                {
                    "type": "fixErrors",
                    "strategy": "autoFix",
                    "onSuccess": "RunTests",
                    "onFailure": "NotifyUser"
                }
            ]
        },
        {
            "name": "HandleTestFailures",
            "trigger": "onTestFailure",
            "actions": [
                {
                    "type": "notify",
                    "message": "Tests failed. Please review the test results."
                }
            ]
        },
        {
            "name": "HandleTypeErrors",
            "trigger": "onAnalysisFailure",
            "actions": [
                {
                    "type": "notify",
                    "message": "Type checking found issues. Please review."
                }
            ]
        },
        {
            "name": "AutoReview",
            "trigger": "onAnalysisSuccess",
            "actions": [
                {
                    "type": "codeReview",
                    "model": "deepseek-r1",
                    "onSuccess": "PresentForAcceptance",
                    "onFailure": "NotifyUser"
                }
            ]
        },
        {
            "name": "PresentForAcceptance",
            "trigger": "onReviewSuccess",
            "actions": [
                {
                    "type": "present",
                    "message": "Code is error-free and has passed all checks. Please accept the changes."
                }
            ]
        },
        {
            "name": "NotifyUser",
            "trigger": "onFailure",
            "actions": [
                {
                    "type": "notify",
                    "message": "An issue occurred during the automated process. Please check manually."
                }
            ]
        },
        {
            "name": "JupyterValidation",
            "trigger": "onNotebookSave",
            "actions": [
                {
                    "type": "validate",
                    "command": ".venv/bin/python -m scripts.validate_notebooks",
                    "onSuccess": "NotifySuccess",
                    "onFailure": "FixNotebook"
                }
            ]
        },
        {
            "name": "FixNotebook",
            "trigger": "onValidationFailure",
            "actions": [
                {
                    "type": "notify",
                    "message": "Notebook validation failed. Attempting to fix..."
                },
                {
                    "type": "fixErrors",
                    "command": ".venv/bin/python -m scripts.fix_notebooks",
                    "onSuccess": "NotifySuccess",
                    "onFailure": "NotifyUser"
                }
            ]
        },
        {
            "name": "NotifySuccess",
            "trigger": "onValidationSuccess",
            "actions": [
                {
                    "type": "notify",
                    "message": "Notebook is properly formatted and compatible with Spark/Cursor."
                }
            ]
        }
    ],
    "cursorTools": {
        "commands": {
            "web": {
                "description": "Get answers from the web using Perplexity AI",
                "usage": "cursor-tools web \"<your question>\"",
                "example": "cursor-tools web \"latest weather in London\"",
                "note": "For complex queries, write output to local-research/<query summary>.md"
            },
            "repo": {
                "description": "Get context-aware answers about this repository using Google Gemini",
                "usage": "cursor-tools repo \"<your question>\"",
                "example": "cursor-tools repo \"explain authentication flow\""
            },
            "doc": {
                "description": "Generate comprehensive documentation for this repository",
                "usage": "cursor-tools doc [options]",
                "example": "cursor-tools doc --output docs.md",
                "note": "For remote repos, write output to local-docs/<repo-name>.md"
            },
            "github": {
                "pr": {
                    "description": "Get PRs",
                    "usage": "cursor-tools github pr [number]",
                    "example": "cursor-tools github pr 123"
                },
                "issue": {
                    "description": "Get issues",
                    "usage": "cursor-tools github issue [number]",
                    "example": "cursor-tools github issue 456"
                }
            },
            "browser": {
                "open": {
                    "description": "Open a URL and capture page content",
                    "usage": "cursor-tools browser open <url> [options]",
                    "example": "cursor-tools browser open \"https://example.com\" --html"
                },
                "act": {
                    "description": "Execute actions on a webpage",
                    "usage": "cursor-tools browser act \"<instruction>\" --url=<url> [options]",
                    "example": "cursor-tools browser act \"Click Login\" --url=https://example.com"
                },
                "observe": {
                    "description": "Observe interactive elements on a webpage",
                    "usage": "cursor-tools browser observe \"<instruction>\" --url=<url> [options]",
                    "example": "cursor-tools browser observe \"interactive elements\" --url=https://example.com"
                },
                "extract": {
                    "description": "Extract data from a webpage",
                    "usage": "cursor-tools browser extract \"<instruction>\" --url=<url> [options]",
                    "example": "cursor-tools browser extract \"product names\" --url=https://example.com/products"
                }
            }
        },
        "options": {
            "general": {
                "model": "Specify alternative AI model",
                "maxTokens": "Control response length",
                "saveTo": "Save command output to file"
            },
            "documentation": {
                "fromGithub": "Generate docs for remote GitHub repository"
            },
            "browser": {
                "console": "Capture browser console logs",
                "html": "Capture page HTML content",
                "network": "Capture network activity",
                "screenshot": "Save page screenshot",
                "timeout": "Set navigation timeout",
                "viewport": "Set viewport size",
                "headless": "Run browser in headless mode",
                "connectTo": "Connect to existing Chrome instance",
                "wait": "Wait after page load",
                "video": "Save video recording"
            }
        },
        "help": {
            "webSearch": {
                "description": "Get answers from the web using Perplexity AI",
                "usage": "cursor-tools web \"<your question>\"",
                "example": "cursor-tools web \"latest weather in London\"",
                "note": "For complex queries, write output to local-research/<query summary>.md"
            },
            "repoContext": {
                "description": "Get context-aware answers about this repository using Google Gemini",
                "usage": "cursor-tools repo \"<your question>\"",
                "example": "cursor-tools repo \"explain authentication flow\""
            },
            "docGeneration": {
                "description": "Generate comprehensive documentation for this repository",
                "usage": "cursor-tools doc [options]",
                "example": "cursor-tools doc --output docs.md",
                "note": "For remote repos, write output to local-docs/<repo-name>.md"
            },
            "githubInfo": {
                "pr": {
                    "description": "Get PRs",
                    "usage": "cursor-tools github pr [number]",
                    "example": "cursor-tools github pr 123"
                },
                "issue": {
                    "description": "Get issues",
                    "usage": "cursor-tools github issue [number]",
                    "example": "cursor-tools github issue 456"
                }
            },
            "browserAutomation": {
                "notes": [
                    "All browser commands are stateless",
                    "Multi-step workflows supported",
                    "Video recording available",
                    "Wait command disabled"
                ],
                "commands": {
                    "open": {
                        "description": "Open URL and capture content",
                        "usage": "cursor-tools browser open <url> [options]"
                    },
                    "act": {
                        "description": "Execute webpage actions",
                        "usage": "cursor-tools browser act \"<instruction>\" --url=<url>"
                    },
                    "observe": {
                        "description": "Observe webpage elements",
                        "usage": "cursor-tools browser observe \"<instruction>\" --url=<url>"
                    },
                    "extract": {
                        "description": "Extract webpage data",
                        "usage": "cursor-tools browser extract \"<instruction>\" --url=<url>"
                    }
                }
            },
            "toolRecommendations": {
                "web": "Best for general web information",
                "repo": "Ideal for repository-specific questions",
                "doc": "Generates repository documentation",
                "browser": "Useful for web app testing"
            },
            "runningCommands": {
                "installed": "Use cursor-tools <command> or package manager commands",
                "uninstalled": "Use npx/bunx with cursor-tools@latest"
            },
            "installation": {
                "config": "cursor-tools.config.json location",
                "apiKeys": ".cursor-tools.env location",
                "requirements": "Playwright required for browser commands"
            }
        }
    }
}
</file>

<file path=".gitignore">
# Maven
target/
pom.xml.tag
pom.xml.releaseBackup
pom.xml.versionsBackup
pom.xml.next
release.properties
dependency-reduced-pom.xml
buildNumber.properties
.mvn/timing.properties

# IDE
.idea/
*.iml
.vscode/
.settings/
.project
.classpath

# Compiled files
*.class
*.jar
*.war
*.ear
*.zip
*.tar.gz
*.rar

# Logs
*.log
logs/

# OS specific
.DS_Store
Thumbs.db 

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
.env
.venv
env/
venv/
ENV/

# Jupyter Notebook
.ipynb_checkpoints
*/.ipynb_checkpoints/*
*.ipynb_metadata

# IDE
.idea/
*.iml
.vscode/
.settings/
.project
.classpath

# Neo4j
data/
logs/
import/
plugins/
neo4j.conf
.neo4j/
neo4j-data/

# Project specific
tmp/
.stash/
*.bak
*.tmp
*.log

# Credentials and secrets
*.pem
*.key
*.cert
.env*
!.env.example

# Cursor rules
!.cursor*

# Data files
*.csv
*.json
!data_model.json
!example_data/*.csv
!example_data/*.json

# Documentation build
_build/
_static/
_templates/
</file>

<file path="check_environment.sh">
#!/bin/bash
# Function to check if SSH key is added to ssh-agent
check_ssh_key() {
    local key_path="$HOME/.ssh/id_ed25519_git_private2"
    # Check if key exists
    if [ ! -f "$key_path" ]; then
        echo "Error: SSH key not found at $key_path"
        return 1
    fi
    # Check if ssh-agent is running
    if ! pgrep -u "$USER" ssh-agent > /dev/null; then
        eval "$(ssh-agent -s)"
    fi
    # Check if key is already added
    if ! ssh-add -l | grep -q "$key_path"; then
        echo "Adding SSH key to ssh-agent..."
        ssh-add "$key_path"
    fi
    return 0
}
# Function to check virtual environment
check_venv() {
    if [ ! -d ".venv" ]; then
        echo "Error: Virtual environment not found. Please run setup_venv.sh first."
        return 1
    fi
    # Check if venv is activated
    if [ -z "$VIRTUAL_ENV" ]; then
        echo "Activating virtual environment..."
        source .venv/bin/activate
    fi
    return 0
}
# Main check function
main() {
    local check_failed=0
    echo "Checking environment..."
    # Check SSH key
    if ! check_ssh_key; then
        check_failed=1
    fi
    # Check virtual environment
    if ! check_venv; then
        check_failed=1
    fi
    if [ $check_failed -eq 1 ]; then
        echo "Environment check failed. Please fix the issues above."
        exit 1
    fi
    echo "Environment check passed!"
    return 0
}
# Run main function
main
</file>

<file path="data_model.md">
# Neo4j Movie Graph Data Model

## Node Labels
1. **Movie** - Represents a film
   - Properties: id, title, release_date, budget, revenue, popularity, vote_average, vote_count, overview
   
2. **Person** - Represents people involved in movies
   - Properties: id, name, gender, profile_path, popularity
   
3. **Genre** - Represents movie categories
   - Properties: id, name
   
4. **Keyword** - Represents movie keywords/tags
   - Properties: id, name
   
5. **Company** - Represents production companies
   - Properties: id, name, origin_country

## Relationships
1. **ACTED_IN** - Person to Movie
   - Properties: character, order
   
2. **DIRECTED** - Person to Movie
   - Properties: job, department
   
3. **PRODUCED** - Company to Movie
   - No properties
   
4. **CATEGORIZED_AS** - Movie to Genre
   - No properties
   
5. **TAGGED_WITH** - Movie to Keyword
   - No properties
   
6. **SIMILAR_TO** - Movie to Movie
   - Properties: similarity_score
   
7. **WORKED_WITH** - Person to Person
   - Properties: movie_count

## Example Cypher Queries

### Creating Nodes
```cypher
// Create Movie nodes
CREATE (m:Movie {id: 550, title: 'Fight Club', release_date: '1999-10-15', budget: 63000000, revenue: 100853753, popularity: 0.5, vote_average: 8.3, vote_count: 3439})

// Create Person nodes
CREATE (p:Person {id: 819, name: 'Edward Norton', gender: 1})

// Create Genre nodes
CREATE (g:Genre {id: 18, name: 'Drama'})
```

### Creating Relationships
```cypher
// Actor relationship
MATCH (p:Person {id: 819}), (m:Movie {id: 550})
CREATE (p)-[:ACTED_IN {character: 'The Narrator', order: 0}]->(m)

// Director relationship
MATCH (p:Person {id: 7467}), (m:Movie {id: 550})
CREATE (p)-[:DIRECTED {job: 'Director', department: 'Directing'}]->(m)

// Genre relationship
MATCH (m:Movie {id: 550}), (g:Genre {id: 18})
CREATE (m)-[:CATEGORIZED_AS]->(g)
```

### Sample Queries for Analysis
```cypher
// Find all movies a person acted in
MATCH (p:Person {name: 'Edward Norton'})-[r:ACTED_IN]->(m:Movie)
RETURN p.name, m.title, r.character

// Find who directed movies in a specific genre
MATCH (p:Person)-[:DIRECTED]->(m:Movie)-[:CATEGORIZED_AS]->(g:Genre {name: 'Drama'})
RETURN p.name, count(m) AS movie_count
ORDER BY movie_count DESC
LIMIT 10

// Find similar movies based on shared actors
MATCH (m1:Movie {title: 'Fight Club'})<-[:ACTED_IN]-(p:Person)-[:ACTED_IN]->(m2:Movie)
WHERE m1 <> m2
RETURN m2.title, count(p) AS shared_actors
ORDER BY shared_actors DESC
LIMIT 10

// Movie recommendation based on genre and ratings
MATCH (m1:Movie {title: 'Fight Club'})-[:CATEGORIZED_AS]->(g:Genre)<-[:CATEGORIZED_AS]-(m2:Movie)
WHERE m1 <> m2 AND m2.vote_average > 7.5
RETURN m2.title, m2.vote_average, collect(g.name) AS shared_genres
ORDER BY m2.vote_average DESC
LIMIT 10
```
</file>

<file path="README.md">
# Neo4j Movie Analysis

A Python project for analyzing movie datasets using the Neo4j graph database. This project includes ETL processing, data visualization, and graph analysis using Jupyter notebooks.

## Project Overview

This project demonstrates how to:

1. Transform movie datasets into Neo4j graph structure
2. Import data into Neo4j database
3. Query and analyze the graph using Cypher
4. Visualize relationships between movies, actors, directors, and genres

## Setup Instructions

### Prerequisites

- Python 3.11+
- Neo4j Database (local or cloud)
- Movie dataset (compatible with TMDB format)

### Installation

1. Clone the repository:
   ```
   git clone <repository-url>
   cd neo4j-movie-analysis
   ```

2. Create and activate virtual environment:
   ```
   ./setup_venv.sh
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. Install dependencies (if not using the setup script):
   ```
   pip install -r requirements.txt
   ```

4. Set up Neo4j:
   - Install Neo4j or use Neo4j Aura (cloud)
   - Create a new database
   - Note your connection URL and credentials

### Data Preparation

1. Place your raw movie data in the `data/raw/` directory:
   - `tmdb_5000_movies.csv` - Movie information
   - `tmdb_5000_credits.csv` - Movie credits data

2. Run the ETL process:
   ```
   python scripts/etl_process.py --input data/raw --output data/processed
   ```

3. The processed data will be saved to `data/processed/` ready for Neo4j import

## Environment Checks

This project includes automatic environment checks that run before executing Python scripts. These checks ensure:

1. The SSH key (`~/.ssh/id_ed25519_git_private2`) is properly loaded in the ssh-agent
2. The virtual environment is set up and activated

### Running Scripts with Environment Checks

Use the `run_with_checks.py` wrapper to execute Python scripts:

```bash
python scripts/run_with_checks.py <script_path> [args...]
```

Example:
```bash
python scripts/run_with_checks.py scripts/etl_process.py --input data/raw --output data/processed
```

The wrapper will:
1. Verify the SSH key is loaded
2. Check and activate the virtual environment if needed
3. Execute the specified script with any provided arguments

If any checks fail, the script execution will be blocked until the issues are resolved.

## Project Structure

```
neo4j-movie-analysis/
 data/                   # Data directory
    raw/                # Raw data files
    processed/          # Processed data for Neo4j import
 movie_graph/            # Main package
    __init__.py         # Package initialization
    etl/                # ETL code
       __init__.py
       process.py      # ETL processing code
    db/                 # Database connectivity
       __init__.py
       connection.py   # Neo4j connection class
    utils/              # Utility functions
        __init__.py
        helpers.py      # Helper functions
 notebooks/              # Jupyter notebooks
    movie_analysis.ipynb        # Original notebook
    movie_analysis_updated.ipynb  # Updated notebook using the package API
    cypher/             # Cypher query examples
 scripts/                # Utility scripts
    run_with_checks.py  # Environment check wrapper
    etl_process.py      # ETL processing script
 tests/                  # Test suite
 workflow/               # Project documentation
 README.md               # Project documentation
 requirements.txt        # Dependencies
 setup_venv.sh           # Environment setup script
 check_environment.sh    # Environment check script
 run_tests.sh            # Test runner script
```
</file>

<file path="requirements.txt">
neo4j>=5.11.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
jupyter>=1.0.0
notebook>=7.0.0
pytest>=7.0.0
flake8>=6.0.0
mypy>=1.0.0
black>=23.0.0
python-dotenv>=1.0.0
coverage>=7.0.0
numpy>=1.24.0
</file>

<file path="run_tests.sh">
#!/bin/bash
# Script to run tests with coverage reporting
# Exit on error
set -e
# Activate virtual environment if not already activated
if [ -z "$VIRTUAL_ENV" ]; then
    echo "Activating virtual environment..."
    source .venv/bin/activate
fi
# Run tests with coverage
echo "Running tests with coverage..."
python -m pytest tests/ --cov=movie_graph --cov-report=term --cov-report=html
echo "Tests completed!"
echo "See htmlcov/index.html for coverage report."
</file>

<file path="setup_venv.sh">
#!/bin/bash
# Script to set up a new virtual environment for the project
# Exit on error
set -e
echo "Setting up virtual environment..."
# Remove existing venv if it exists
if [ -d ".venv" ]; then
    echo "Removing existing virtual environment..."
    rm -rf .venv
fi
# Create new virtual environment
echo "Creating new virtual environment..."
python3 -m venv .venv
# Activate virtual environment
echo "Activating virtual environment..."
source .venv/bin/activate
# Upgrade pip
echo "Upgrading pip..."
pip install --upgrade pip
# Install dependencies
echo "Installing dependencies..."
pip install -r requirements.txt
# Optional: Install development dependencies
echo "Installing development dependencies..."
pip install pytest pytest-cov black flake8 mypy
echo "Virtual environment setup complete!"
echo "To activate the virtual environment, run:"
echo "source .venv/bin/activate"
</file>

</files>
